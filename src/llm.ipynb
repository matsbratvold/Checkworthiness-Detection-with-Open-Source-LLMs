{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check-worthiness detection using Large Language Models\n",
    "\n",
    "First, the necessary python modules are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/.conda/envs/fakeNews/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-03 18:48:48.343057: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-03 18:48:52.102937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 18:48:52.121608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 18:48:52.650741: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-03 18:48:53.598618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 18:49:04.765523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from claimbuster_utils import load_claimbuster_dataset\n",
    "from checkthat_utils import load_check_that_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.03s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config = bnb_config,\n",
    "    # attn_implementation=\"flash_attention_2\", \n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClaimBuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27247</th>\n",
       "      <td>1</td>\n",
       "      <td>We're 9 million jobs short of that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, last year up to this time, we've los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>1</td>\n",
       "      <td>And in November of 1975 I was the first presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19700</th>\n",
       "      <td>1</td>\n",
       "      <td>And what we've done during the Bush administra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you know we don't have a single program spo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Verdict                                               Text\n",
       "sentence_id                                                            \n",
       "27247              1                We're 9 million jobs short of that.\n",
       "10766              1  You know, last year up to this time, we've los...\n",
       "3327               1  And in November of 1975 I was the first presid...\n",
       "19700              1  And what we've done during the Bush administra...\n",
       "12600              1  Do you know we don't have a single program spo..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 298/9674 [01:42<39:52,  3.92it/s]  /tmp/ipykernel_4125055/2661599538.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
      "100%|██████████| 9674/9674 [31:28<00:00,  5.12it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../prompts/ClaimBuster/standard/zero-shot.txt\", \"r\") as f:\n",
    "    instruction = f.read().replace(\"\\n\", \"\")\n",
    "use_contextual = False\n",
    "data = load_claimbuster_dataset(\n",
    "    \"../data/ClaimBuster_Datasets/datasets\",\n",
    "    use_contextual_features=use_contextual,\n",
    "    debate_transcripts_folder=\"../data/ClaimBuster_Datasets/debate_transcripts\",\n",
    ")\n",
    "\n",
    "texts = data[\"Text\"]\n",
    "if use_contextual is False:\n",
    "    prompts = [f\"{instruction} '''{text}'''\" for text in texts]\n",
    "    zeroshot_output = \"../results/ClaimBuster/zeroshot1.csv\"\n",
    "else:\n",
    "    contexts = data[\"previous_sentences\"].tolist()\n",
    "    prompts = [\n",
    "        f\"{instruction} For context, the following senteces were said prior to the one in question: {context} Only evaluate the check-worthiness of the following sentence: '''{text}'''\"\n",
    "        for text, context in zip(texts, contexts)\n",
    "    ]\n",
    "    zeroshot_output = \"../results/ClaimBuster/zeroshot_contextual.csv\"\n",
    "\n",
    "\n",
    "class ProgressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "prompts_data = ProgressDataset(prompts)\n",
    "\n",
    "dataset_with_scores = data.copy()\n",
    "\n",
    "display(data.head())\n",
    "dict_matcher = re.compile(r\"{.*}\")\n",
    "score_matcher = re.compile(r\"([Ss]core[^\\d]*)(\\d+)\")\n",
    "non_check_worthy_matcher = re.compile(\n",
    "    r\"(non-checkworthy)|(not check-worthy)|(non check-worthy)\"\n",
    ")\n",
    "\n",
    "responses = pipe(prompts_data, batch_size=128)\n",
    "for index, result in enumerate(tqdm(responses, total=len(prompts))):\n",
    "    response = result[0][\"generated_text\"].replace(\"\\n\", \"\")\n",
    "    dataset_index = data.index[index]\n",
    "    try:\n",
    "        parsed_json = json.loads(dict_matcher.search(response).group(0))\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = parsed_json[\"score\"]\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = parsed_json[\"reasoning\"]\n",
    "    except (json.decoder.JSONDecodeError, AttributeError) as e:\n",
    "        # Try to find score\n",
    "        score = score_matcher.search(response)\n",
    "        if score is not None:\n",
    "            score = score[2]\n",
    "        else:\n",
    "            score = 0.0 if non_check_worthy_matcher.search(response) else np.nan\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = response\n",
    "        continue\n",
    "# Set the following column order: Verdict, score, Text, reasoning, previous_sentences\n",
    "columns =  [\"Verdict\", \"score\", \"Text\", \"reasoning\"]\n",
    "if use_contextual:\n",
    "    columns.append(\"previous_sentences\")\n",
    "dataset_with_scores = dataset_with_scores[columns]\n",
    "dataset_with_scores.to_csv(zeroshot_output, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.28376102, 1.50472569, 1.22792745, 1.2808435 ]), 'score_time': array([0.00417757, 0.0038116 , 0.00471807, 0.00366807]), 'test_f1_macro': array([0.64137709, 0.63468915, 0.62384077, 0.62942657]), 'test_accuracy': array([0.68168665, 0.68582059, 0.66956162, 0.67162945])}\n"
     ]
    }
   ],
   "source": [
    "# Print the number of empty scores\n",
    "dataset_path = \"../results/ClaimBuster/zeroshot_contextual.csv\"\n",
    "dataset_with_scores = pd.read_csv(dataset_path, index_col=0)\n",
    "class ThresholdOptimizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "\n",
    "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
    "        \n",
    "        y_gold = x[\"Verdict\"].values\n",
    "\n",
    "        reports = []\n",
    "        for threshold in range(1, 100):\n",
    "            y_pred = x[\"score\"].map(lambda x: 1 if x >= threshold else 0).values\n",
    "            report = classification_report(y_gold, y_pred, output_dict=True)\n",
    "            report[\"threshold\"] = threshold\n",
    "            reports.append(report)\n",
    "        self.threshold = max(reports, key=lambda report: report[\"macro avg\"][\"f1-score\"])[\"threshold\"]\n",
    "    \n",
    "    def predict(self, x: pd.DataFrame):\n",
    "        predictions = x[\"score\"].map(lambda x: 1 if x >= self.threshold else 0)\n",
    "        return predictions\n",
    "\n",
    "# Do a four fold cross validation where the threshold is optimized\n",
    "print(cross_validate(\n",
    "    ThresholdOptimizer(),\n",
    "    X=dataset_with_scores,\n",
    "    y=dataset_with_scores[\"Verdict\"],\n",
    "    cv=StratifiedKFold(n_splits=4),\n",
    "    scoring=[\"f1_macro\", \"accuracy\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheckThat 2021 Task 1a Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>claim</th>\n",
       "      <th>check_worthiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1234964653014384644</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/EricTrump/status/123496465...</td>\n",
       "      <td>Since this will never get reported by the medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234869939720216578</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/RealJamesWoods/status/1234...</td>\n",
       "      <td>Thanks, #MichaelBloomberg. Here’s a handy litt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234873136304267267</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/hayxsmith/status/123487313...</td>\n",
       "      <td>Folks, when you say \"The corona virus isn't a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235071285027147776</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/ipspankajnain/status/12350...</td>\n",
       "      <td>Just 1 case of Corona Virus in India and  peop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234911110861594624</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/PressSec/status/1234911110...</td>\n",
       "      <td>President  @realDonaldTrump  made a commitment...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     topic_id  \\\n",
       "tweet_id                        \n",
       "1234964653014384644  covid-19   \n",
       "1234869939720216578  covid-19   \n",
       "1234873136304267267  covid-19   \n",
       "1235071285027147776  covid-19   \n",
       "1234911110861594624  covid-19   \n",
       "\n",
       "                                                             tweet_url  \\\n",
       "tweet_id                                                                 \n",
       "1234964653014384644  https://twitter.com/EricTrump/status/123496465...   \n",
       "1234869939720216578  https://twitter.com/RealJamesWoods/status/1234...   \n",
       "1234873136304267267  https://twitter.com/hayxsmith/status/123487313...   \n",
       "1235071285027147776  https://twitter.com/ipspankajnain/status/12350...   \n",
       "1234911110861594624  https://twitter.com/PressSec/status/1234911110...   \n",
       "\n",
       "                                                            tweet_text  claim  \\\n",
       "tweet_id                                                                        \n",
       "1234964653014384644  Since this will never get reported by the medi...      1   \n",
       "1234869939720216578  Thanks, #MichaelBloomberg. Here’s a handy litt...      0   \n",
       "1234873136304267267  Folks, when you say \"The corona virus isn't a ...      0   \n",
       "1235071285027147776  Just 1 case of Corona Virus in India and  peop...      1   \n",
       "1234911110861594624  President  @realDonaldTrump  made a commitment...      1   \n",
       "\n",
       "                     check_worthiness  \n",
       "tweet_id                               \n",
       "1234964653014384644                 1  \n",
       "1234869939720216578                 0  \n",
       "1234873136304267267                 0  \n",
       "1235071285027147776                 0  \n",
       "1234911110861594624                 1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1172 [00:46<15:14:45, 46.87s/it]/tmp/ipykernel_10891/1019148809.py:59: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
      "100%|██████████| 1172/1172 [07:01<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "with open(\"../prompts/ClaimBuster/standard/zero-shot.txt\", \"r\") as f:\n",
    "    instruction = f.read().replace(\"\\n\", \"\")\n",
    "use_contextual = False\n",
    "data = load_check_that_dataset(\n",
    "    \"../data/CheckThat2021Task1a\",\n",
    ")\n",
    "\n",
    "texts = data[\"tweet_text\"]\n",
    "if use_contextual is False:\n",
    "    prompts = [f\"{instruction} '''{text}'''\" for text in texts]\n",
    "    zeroshot_output = \"../results/CheckThat/zeroshot1.csv\"\n",
    "else:\n",
    "    contexts = data[\"previous_sentences\"].tolist()\n",
    "    prompts = [\n",
    "        f\"{instruction} For context, the following senteces were said prior to the one in question: {context} Only evaluate the check-worthiness of the following sentence: '''{text}'''\"\n",
    "        for text, context in zip(texts, contexts)\n",
    "    ]\n",
    "    zeroshot_output = \"../results/Checkthat/zeroshot_contextual.csv\"\n",
    "\n",
    "\n",
    "class ProgressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "prompts_data = ProgressDataset(prompts)\n",
    "\n",
    "dataset_with_scores = data.copy()\n",
    "\n",
    "display(data.head())\n",
    "dict_matcher = re.compile(r\"{.*}\")\n",
    "score_matcher = re.compile(r\"([Ss]core[^\\d]*)(\\d+)\")\n",
    "non_check_worthy_matcher = re.compile(\n",
    "    r\"(non-checkworthy)|(not check-worthy)|(non check-worthy)\"\n",
    ")\n",
    "\n",
    "responses = pipe(prompts_data, batch_size=128)\n",
    "for index, result in enumerate(tqdm(responses, total=len(prompts))):\n",
    "    response = result[0][\"generated_text\"].replace(\"\\n\", \"\")\n",
    "    dataset_index = data.index[index]\n",
    "    try:\n",
    "        parsed_json = json.loads(dict_matcher.search(response).group(0))\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = parsed_json[\"score\"]\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = parsed_json[\"reasoning\"]\n",
    "    except (json.decoder.JSONDecodeError, AttributeError) as e:\n",
    "        # Try to find score\n",
    "        score = score_matcher.search(response)\n",
    "        if score is not None:\n",
    "            score = score[2]\n",
    "        else:\n",
    "            score = 0.0 if non_check_worthy_matcher.search(response) else np.nan\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = response\n",
    "        continue\n",
    "columns =  [\"check_worthiness\", \"score\", \"tweet_text\", \"reasoning\"]\n",
    "if use_contextual:\n",
    "    columns.append(\"previous_sentences\")\n",
    "dataset_with_scores = dataset_with_scores[columns]\n",
    "dataset_with_scores.to_csv(zeroshot_output, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.5817163 , 0.554106  , 0.56280541, 0.57334805]), 'score_time': array([0.00258136, 0.00253034, 0.00249505, 0.00250578]), 'test_f1_macro': array([0.64136111, 0.7040404 , 0.50021622, 0.33657796]), 'test_accuracy': array([0.69283276, 0.74061433, 0.51535836, 0.33788396])}\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../results/CheckThat/zeroshot1.csv\"\n",
    "dataset_with_scores = pd.read_csv(dataset_path, index_col=0)\n",
    "class ThresholdOptimizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "\n",
    "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
    "        \n",
    "        y_gold = x[\"check_worthiness\"].values\n",
    "\n",
    "        reports = []\n",
    "        for threshold in range(1, 100):\n",
    "            y_pred = x[\"score\"].map(lambda x: 1 if x >= threshold else 0).values\n",
    "            report = classification_report(y_gold, y_pred, output_dict=True)\n",
    "            report[\"threshold\"] = threshold\n",
    "            reports.append(report)\n",
    "        self.threshold = max(reports, key=lambda report: report[\"macro avg\"][\"f1-score\"])[\"threshold\"]\n",
    "    \n",
    "    def predict(self, x: pd.DataFrame):\n",
    "        predictions = x[\"score\"].map(lambda x: 1 if x >= self.threshold else 0)\n",
    "        return predictions\n",
    "\n",
    "# Do a four fold cross validation where the threshold is optimized\n",
    "print(cross_validate(\n",
    "    ThresholdOptimizer(),\n",
    "    X=dataset_with_scores,\n",
    "    y=dataset_with_scores[\"check_worthiness\"],\n",
    "    cv=StratifiedKFold(n_splits=4),\n",
    "    scoring=[\"f1_macro\", \"accuracy\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
