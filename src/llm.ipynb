{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check-worthiness detection using Large Language Models\n",
    "\n",
    "First, the necessary python modules are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/.conda/envs/fakeNews/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-04 11:16:40.134769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-04 11:16:45.318784: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 11:16:45.328336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 11:16:45.926964: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 11:16:47.904800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 11:16:59.721966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from claimbuster_utils import load_claimbuster_dataset\n",
    "from checkthat_utils import load_check_that_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config = bnb_config,\n",
    "    # attn_implementation=\"flash_attention_2\", \n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClaimBuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27247</th>\n",
       "      <td>1</td>\n",
       "      <td>We're 9 million jobs short of that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, last year up to this time, we've los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>1</td>\n",
       "      <td>And in November of 1975 I was the first presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19700</th>\n",
       "      <td>1</td>\n",
       "      <td>And what we've done during the Bush administra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you know we don't have a single program spo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Verdict                                               Text\n",
       "sentence_id                                                            \n",
       "27247              1                We're 9 million jobs short of that.\n",
       "10766              1  You know, last year up to this time, we've los...\n",
       "3327               1  And in November of 1975 I was the first presid...\n",
       "19700              1  And what we've done during the Bush administra...\n",
       "12600              1  Do you know we don't have a single program spo..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 298/9674 [01:42<39:52,  3.92it/s]  /tmp/ipykernel_4125055/2661599538.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
      "100%|██████████| 9674/9674 [31:28<00:00,  5.12it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../prompts/ClaimBuster/standard/zero-shot.txt\", \"r\") as f:\n",
    "    instruction = f.read().replace(\"\\n\", \"\")\n",
    "use_contextual = False\n",
    "data = load_claimbuster_dataset(\n",
    "    \"../data/ClaimBuster_Datasets/datasets\",\n",
    "    use_contextual_features=use_contextual,\n",
    "    debate_transcripts_folder=\"../data/ClaimBuster_Datasets/debate_transcripts\",\n",
    ")\n",
    "\n",
    "texts = data[\"Text\"]\n",
    "if use_contextual is False:\n",
    "    prompts = [f\"{instruction} '''{text}'''\" for text in texts]\n",
    "    zeroshot_output = \"../results/ClaimBuster/zeroshot1.csv\"\n",
    "else:\n",
    "    contexts = data[\"previous_sentences\"].tolist()\n",
    "    prompts = [\n",
    "        f\"{instruction} For context, the following senteces were said prior to the one in question: {context} Only evaluate the check-worthiness of the following sentence: '''{text}'''\"\n",
    "        for text, context in zip(texts, contexts)\n",
    "    ]\n",
    "    zeroshot_output = \"../results/ClaimBuster/zeroshot_contextual.csv\"\n",
    "\n",
    "\n",
    "class ProgressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "prompts_data = ProgressDataset(prompts)\n",
    "\n",
    "dataset_with_scores = data.copy()\n",
    "\n",
    "display(data.head())\n",
    "dict_matcher = re.compile(r\"{.*}\")\n",
    "score_matcher = re.compile(r\"([Ss]core[^\\d]*)(\\d+)\")\n",
    "non_check_worthy_matcher = re.compile(\n",
    "    r\"(non-checkworthy)|(not check-worthy)|(non check-worthy)\"\n",
    ")\n",
    "\n",
    "responses = pipe(prompts_data, batch_size=128)\n",
    "for index, result in enumerate(tqdm(responses, total=len(prompts))):\n",
    "    response = result[0][\"generated_text\"].replace(\"\\n\", \"\")\n",
    "    dataset_index = data.index[index]\n",
    "    try:\n",
    "        parsed_json = json.loads(dict_matcher.search(response).group(0))\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = parsed_json[\"score\"]\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = parsed_json[\"reasoning\"]\n",
    "    except (json.decoder.JSONDecodeError, AttributeError) as e:\n",
    "        # Try to find score\n",
    "        score = score_matcher.search(response)\n",
    "        if score is not None:\n",
    "            score = score[2]\n",
    "        else:\n",
    "            score = 0.0 if non_check_worthy_matcher.search(response) else np.nan\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = response\n",
    "        continue\n",
    "# Set the following column order: Verdict, score, Text, reasoning, previous_sentences\n",
    "columns =  [\"Verdict\", \"score\", \"Text\", \"reasoning\"]\n",
    "if use_contextual:\n",
    "    columns.append(\"previous_sentences\")\n",
    "dataset_with_scores = dataset_with_scores[columns]\n",
    "dataset_with_scores.to_csv(zeroshot_output, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.28376102, 1.50472569, 1.22792745, 1.2808435 ]), 'score_time': array([0.00417757, 0.0038116 , 0.00471807, 0.00366807]), 'test_f1_macro': array([0.64137709, 0.63468915, 0.62384077, 0.62942657]), 'test_accuracy': array([0.68168665, 0.68582059, 0.66956162, 0.67162945])}\n"
     ]
    }
   ],
   "source": [
    "# Print the number of empty scores\n",
    "dataset_path = \"../results/ClaimBuster/zeroshot_contextual.csv\"\n",
    "dataset_with_scores = pd.read_csv(dataset_path, index_col=0)\n",
    "class ThresholdOptimizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "\n",
    "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
    "        \n",
    "        y_gold = x[\"Verdict\"].values\n",
    "\n",
    "        reports = []\n",
    "        for threshold in range(1, 100):\n",
    "            y_pred = x[\"score\"].map(lambda x: 1 if x >= threshold else 0).values\n",
    "            report = classification_report(y_gold, y_pred, output_dict=True)\n",
    "            report[\"threshold\"] = threshold\n",
    "            reports.append(report)\n",
    "        self.threshold = max(reports, key=lambda report: report[\"macro avg\"][\"f1-score\"])[\"threshold\"]\n",
    "    \n",
    "    def predict(self, x: pd.DataFrame):\n",
    "        predictions = x[\"score\"].map(lambda x: 1 if x >= self.threshold else 0)\n",
    "        return predictions\n",
    "\n",
    "# Do a four fold cross validation where the threshold is optimized\n",
    "print(cross_validate(\n",
    "    ThresholdOptimizer(),\n",
    "    X=dataset_with_scores,\n",
    "    y=dataset_with_scores[\"Verdict\"],\n",
    "    cv=StratifiedKFold(n_splits=4),\n",
    "    scoring=[\"f1_macro\", \"accuracy\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheckThat 2021 Task 1a Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction='You are a fact-checking assistant who are identifying check-worthy tweets.A tweet is check-worthy if is contains a verifiable factual claim. This includes tweets that state a definition, mention a quantity in the present or the past, make a verifiable prediction about the future, reference laws, procedures, and rules of operation, discuss images or videos, and state correlation or causation, among others.Additionally, a tweet is only check-worthy if you think that a professional fact-checkershould verify the claim in the tweet. This implies that the tweet appears to contain false information, it could have an impact on the general public, and the tweet could be harmful to the society, certain people, companies or products.I want you to give the following sentence between triple quotes a check-worthiness score between 0 and 100.Answer using the following json format: {\"score\": [0-100], \"reasoning\": str}'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>claim</th>\n",
       "      <th>check_worthiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1234964653014384644</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/EricTrump/status/123496465...</td>\n",
       "      <td>Since this will never get reported by the medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234869939720216578</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/RealJamesWoods/status/1234...</td>\n",
       "      <td>Thanks, #MichaelBloomberg. Here’s a handy litt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234873136304267267</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/hayxsmith/status/123487313...</td>\n",
       "      <td>Folks, when you say \"The corona virus isn't a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235071285027147776</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/ipspankajnain/status/12350...</td>\n",
       "      <td>Just 1 case of Corona Virus in India and  peop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234911110861594624</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>https://twitter.com/PressSec/status/1234911110...</td>\n",
       "      <td>President  @realDonaldTrump  made a commitment...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     topic_id  \\\n",
       "tweet_id                        \n",
       "1234964653014384644  covid-19   \n",
       "1234869939720216578  covid-19   \n",
       "1234873136304267267  covid-19   \n",
       "1235071285027147776  covid-19   \n",
       "1234911110861594624  covid-19   \n",
       "\n",
       "                                                             tweet_url  \\\n",
       "tweet_id                                                                 \n",
       "1234964653014384644  https://twitter.com/EricTrump/status/123496465...   \n",
       "1234869939720216578  https://twitter.com/RealJamesWoods/status/1234...   \n",
       "1234873136304267267  https://twitter.com/hayxsmith/status/123487313...   \n",
       "1235071285027147776  https://twitter.com/ipspankajnain/status/12350...   \n",
       "1234911110861594624  https://twitter.com/PressSec/status/1234911110...   \n",
       "\n",
       "                                                            tweet_text  claim  \\\n",
       "tweet_id                                                                        \n",
       "1234964653014384644  Since this will never get reported by the medi...      1   \n",
       "1234869939720216578  Thanks, #MichaelBloomberg. Here’s a handy litt...      0   \n",
       "1234873136304267267  Folks, when you say \"The corona virus isn't a ...      0   \n",
       "1235071285027147776  Just 1 case of Corona Virus in India and  peop...      1   \n",
       "1234911110861594624  President  @realDonaldTrump  made a commitment...      1   \n",
       "\n",
       "                     check_worthiness  \n",
       "tweet_id                               \n",
       "1234964653014384644                 1  \n",
       "1234869939720216578                 0  \n",
       "1234873136304267267                 0  \n",
       "1235071285027147776                 0  \n",
       "1234911110861594624                 1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1172 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1172 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m non_check_worthy_matcher \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(non-checkworthy)|(not check-worthy)|(non check-worthy)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m responses \u001b[38;5;241m=\u001b[39m pipe(prompts_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/generation/utils.py:2467\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2464\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2468\u001b[0m     this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:130\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStoppingCriteriaList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(criteria(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_length\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "with open(\"../prompts/CheckThat/standard/zero-shot.txt\", \"r\") as f:\n",
    "    instruction = f.read().replace(\"\\n\", \"\")\n",
    "    print(f\"{instruction=}\")\n",
    "use_contextual = False\n",
    "data = load_check_that_dataset(\n",
    "    \"../data/CheckThat2021Task1a\",\n",
    ")\n",
    "\n",
    "texts = data[\"tweet_text\"]\n",
    "if use_contextual is False:\n",
    "    prompts = [f\"{instruction} '''{text}'''\" for text in texts]\n",
    "    zeroshot_output = \"../results/CheckThat/zeroshot1.csv\"\n",
    "else:\n",
    "    contexts = data[\"previous_sentences\"].tolist()\n",
    "    prompts = [\n",
    "        f\"{instruction} For context, the following senteces were said prior to the one in question: {context} Only evaluate the check-worthiness of the following sentence: '''{text}'''\"\n",
    "        for text, context in zip(texts, contexts)\n",
    "    ]\n",
    "    zeroshot_output = \"../results/Checkthat/zeroshot_contextual.csv\"\n",
    "\n",
    "\n",
    "class ProgressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "prompts_data = ProgressDataset(prompts)\n",
    "\n",
    "dataset_with_scores = data.copy()\n",
    "\n",
    "display(data.head())\n",
    "dict_matcher = re.compile(r\"{.*}\")\n",
    "score_matcher = re.compile(r\"([Ss]core[^\\d]*)(\\d+)\")\n",
    "non_check_worthy_matcher = re.compile(\n",
    "    r\"(non-checkworthy)|(not check-worthy)|(non check-worthy)\"\n",
    ")\n",
    "\n",
    "responses = pipe(prompts_data, batch_size=128)\n",
    "for index, result in enumerate(tqdm(responses, total=len(prompts))):\n",
    "    response = result[0][\"generated_text\"].replace(\"\\n\", \"\")\n",
    "    dataset_index = data.index[index]\n",
    "    try:\n",
    "        parsed_json = json.loads(dict_matcher.search(response).group(0))\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = parsed_json[\"score\"]\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = parsed_json[\"reasoning\"]\n",
    "    except (json.decoder.JSONDecodeError, AttributeError) as e:\n",
    "        # Try to find score\n",
    "        score = score_matcher.search(response)\n",
    "        if score is not None:\n",
    "            score = score[2]\n",
    "        else:\n",
    "            score = 0.0 if non_check_worthy_matcher.search(response) else np.nan\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = response\n",
    "        continue\n",
    "columns =  [\"check_worthiness\", \"score\", \"tweet_text\", \"reasoning\"]\n",
    "if use_contextual:\n",
    "    columns.append(\"previous_sentences\")\n",
    "dataset_with_scores = dataset_with_scores[columns]\n",
    "dataset_with_scores.to_csv(zeroshot_output, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.67098188, 0.55599713, 0.5631063 , 0.64120388]), 'score_time': array([0.03064275, 0.0025475 , 0.00262642, 0.00249791]), 'test_f1_macro': array([0.6522611 , 0.68531064, 0.58840476, 0.52138402]), 'test_accuracy': array([0.70307167, 0.7337884 , 0.59726962, 0.54948805])}\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../results/CheckThat/zeroshot1.csv\"\n",
    "dataset_with_scores = pd.read_csv(dataset_path, index_col=0)\n",
    "class ThresholdOptimizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "\n",
    "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
    "        \n",
    "        y_gold = x[\"check_worthiness\"].values\n",
    "\n",
    "        reports = []\n",
    "        for threshold in range(1, 100):\n",
    "            y_pred = x[\"score\"].map(lambda x: 1 if x >= threshold else 0).values\n",
    "            report = classification_report(y_gold, y_pred, output_dict=True)\n",
    "            report[\"threshold\"] = threshold\n",
    "            reports.append(report)\n",
    "        self.threshold = max(reports, key=lambda report: report[\"macro avg\"][\"f1-score\"])[\"threshold\"]\n",
    "    \n",
    "    def predict(self, x: pd.DataFrame):\n",
    "        predictions = x[\"score\"].map(lambda x: 1 if x >= self.threshold else 0)\n",
    "        return predictions\n",
    "\n",
    "# Do a four fold cross validation where the threshold is optimized\n",
    "print(cross_validate(\n",
    "    ThresholdOptimizer(),\n",
    "    X=dataset_with_scores,\n",
    "    y=dataset_with_scores[\"check_worthiness\"],\n",
    "    cv=StratifiedKFold(n_splits=4),\n",
    "    scoring=[\"f1_macro\", \"accuracy\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
