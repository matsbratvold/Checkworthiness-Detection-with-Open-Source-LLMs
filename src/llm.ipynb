{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check-worthiness detection using Large Language Models\n",
    "\n",
    "First, the necessary python modules are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import os\n",
    "if \"src\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from src.claimbuster_utils import load_claimbuster_dataset\n",
    "from src.checkthat_utils import load_check_that_dataset\n",
    "from src.llm import HuggingFaceModel, run_llm_cross_validation, ICLUsage, PromptType, Experiment\n",
    "from src.result_analysis import generate_error_analysis_report, print_padded_text, create_confusion_matrix\n",
    "from src.dataset_utils import generate_cross_validation_datasets, CustomDataset\n",
    "from src.plotting_utils import show_bar_plot\n",
    "from src.liar_utils import LIARLabel\n",
    "from src.rawfc_utils import RAWFCLabel\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to HuggingFace hub\n",
    "\n",
    "In order to get access to the LLama2 model, you need to login to the Huggingace hub and have gated access to the model. Otherwise this can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check-worthiness predictions (experiments E1-E4)\n",
    "\n",
    "Generates check-worthiness detection predictions using different configurations of LLMs and performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Cross Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "claimbuster = load_claimbuster_dataset(\"../data/ClaimBuster/datasets\")\n",
    "clambuster_datasets = generate_cross_validation_datasets(\n",
    "    data=claimbuster, \n",
    "    folder_path=\"../data/ClaimBuster/crossval\"\n",
    ")\n",
    "\n",
    "checkthat = load_check_that_dataset(\"../data/CheckThat\")\n",
    "checkthat_datasets = generate_cross_validation_datasets(\n",
    "    data=checkthat, \n",
    "    label_column=\"check_worthiness\",\n",
    "    folder_path=\"../data/CheckThat/crossval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions\n",
    "\n",
    "Using ipywidgets to select which model, dataset, and other parameters to generate LLM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# General lauyout\n",
    "input_style = dict(\n",
    "    description_width=\"fit-content\"\n",
    ")\n",
    "\n",
    "# Dataset \n",
    "dataset_select = widgets.Dropdown(\n",
    "    options=[(\"ClaimBuster\", CustomDataset.CLAIMBUSTER), (\"CheckThat\", CustomDataset.CHECK_THAT)],\n",
    "    value=CustomDataset.CLAIMBUSTER,\n",
    "    description=\"Dataset:\"\n",
    ")\n",
    "\n",
    "# Model and parameters\n",
    "model_select = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Mistral 7B Instruct\", HuggingFaceModel.MISTRAL_7B_INSTRUCT), \n",
    "        (\"Mixtral Instruct\", HuggingFaceModel.MIXTRAL_INSTRUCT),\n",
    "        (\"LLama 2 7B Chat\", HuggingFaceModel.LLAMA2_7B_CHAT)],\n",
    "    value=HuggingFaceModel.MISTRAL_7B_INSTRUCT,\n",
    "    description=\"Model:\",\n",
    "    style=input_style\n",
    ")\n",
    "max_new_tokens_int_text = widgets.IntText(\n",
    "    value=64,\n",
    "    description=\"Max new tokens:\",\n",
    "    style=input_style\n",
    ")\n",
    "batch_size = widgets.IntText(\n",
    "    value=32,\n",
    "    description=\"Batch size:\",\n",
    "    style=input_style\n",
    ")\n",
    "model_and_parameters = widgets.VBox(\n",
    "    [model_select, max_new_tokens_int_text, batch_size],\n",
    ")\n",
    "\n",
    "# Prompting type\n",
    "prompting_type = widgets.Dropdown(\n",
    "    options=[(\"Standard\", PromptType.STANDARD), (\"Chain-of-Thought\", PromptType.CHAIN_OF_THOUGHT)],\n",
    "    value=PromptType.STANDARD,\n",
    "    description=\"Prompting type:\",\n",
    "    style=input_style\n",
    ")\n",
    "icl_usage = widgets.Dropdown(\n",
    "    options=[(\"Zero-shot\", ICLUsage.ZERO_SHOT), (\"Few-shot\", ICLUsage.FEW_SHOT)],\n",
    "    value=ICLUsage.ZERO_SHOT,\n",
    "    description=\"ICL usage:\",\n",
    "    style=input_style\n",
    ")\n",
    "prompt_use = widgets.VBox(\n",
    "    [prompting_type, icl_usage]\n",
    ")\n",
    "\n",
    "accordion = widgets.Accordion([\n",
    "    dataset_select,\n",
    "    model_and_parameters,\n",
    "    prompt_use\n",
    "],\n",
    "    titles=[\"Dataset\", \"Model and parameters\", \"Prompting type\"],\n",
    ")\n",
    "\n",
    "title = widgets.HTML(\n",
    "    \"<h1>Generation of predictions using LLMs</h1>\",\n",
    ")\n",
    "description = widgets.HTML(\n",
    "    \"<div>Set the parameters to select what dataset, model and prompting to use when generating predictions. If you experience Cuda out of memory issues, please decrease the batch size.</div>\",\n",
    "    layout={\"font-size\": '14px'}\n",
    ")\n",
    "start_generation_button = widgets.Button(\n",
    "    description=\"Start generation\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    layout={\"height\": \"40px\", \"width\": \"calc(100% - 4px)\"},\n",
    ")\n",
    "\n",
    "def handle_generation_click(_):\n",
    "    \n",
    "\n",
    "    print(\"#\" * 50)\n",
    "    print_padded_text(\"Starting generation with parameters\")\n",
    "    print_padded_text(f\"Dataset: {dataset_select.value.value}\")\n",
    "    print_padded_text(f\"Model: {model_select.value.name}\")\n",
    "    print_padded_text(f\"Prompting type: {prompting_type.value.value}\")\n",
    "    print_padded_text(f\"ICL usage: {icl_usage.value.value}\")\n",
    "    print(\"#\" * 50) \n",
    "    # The generation of prediction is performed in the llm.py script to make sure the GPU resources are disposed of\n",
    "    os.system(\n",
    "        f\"\"\"python3 -m src.llm \\\n",
    "                --dataset={dataset_select.value.value} \\\n",
    "                --prompt-type={prompting_type.value.value} \\\n",
    "                --icl_usage={icl_usage.value.value} \\\n",
    "                --batch-size={batch_size.value} \\\n",
    "                --max-new-tokens={max_new_tokens_int_text.value} \\\n",
    "                --model-id={model_select.value.value}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "start_generation_button.on_click(handle_generation_click)\n",
    "\n",
    "\n",
    "box = widgets.Box(\n",
    "    [title, description, accordion, start_generation_button],\n",
    "    layout=widgets.Layout(\n",
    "        padding= '16px', \n",
    "        display= \"flex\", \n",
    "        flex_flow=\"column\",\n",
    "        align_items=\"stretch\",\n",
    "        border=\"1px solid black\"\n",
    "    )\n",
    ") \n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "Using ipywidgets to select what models to run cross validation on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "input_style = dict(\n",
    "    description_width=\"fit-content\"\n",
    ")\n",
    "\n",
    "dataset_select = widgets.Dropdown(\n",
    "    options=[(\"ClaimBuster\", CustomDataset.CLAIMBUSTER), (\"CheckThat\", CustomDataset.CHECK_THAT)],\n",
    "    value=CustomDataset.CLAIMBUSTER,\n",
    "    description=\"Dataset:\"\n",
    ")\n",
    "\n",
    "model_select = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Mistral 7B Instruct\", HuggingFaceModel.MISTRAL_7B_INSTRUCT), \n",
    "        (\"Mixtral Instruct\", HuggingFaceModel.MIXTRAL_INSTRUCT),\n",
    "        ('LLama2 7B Chat', HuggingFaceModel.LLAMA2_7B_CHAT)\n",
    "    ],\n",
    "    value=HuggingFaceModel.MISTRAL_7B_INSTRUCT,\n",
    "    description=\"Model:\",\n",
    "    style=input_style\n",
    ")\n",
    "\n",
    "prompting_type = widgets.Dropdown(\n",
    "    options=[(\"Standard\", PromptType.STANDARD), (\"Chain-of-Thought\", PromptType.CHAIN_OF_THOUGHT)],\n",
    "    value=PromptType.STANDARD,\n",
    "    description=\"Prompting type:\",\n",
    "    style=input_style\n",
    ")\n",
    "icl_usage = widgets.Dropdown(\n",
    "    options=[(\"Zero-shot\", ICLUsage.ZERO_SHOT), (\"Few-shot\", ICLUsage.FEW_SHOT)],\n",
    "    value=ICLUsage.ZERO_SHOT,\n",
    "    description=\"ICL usage:\",\n",
    "    style=input_style,\n",
    ")\n",
    "\n",
    "prompt_use = widgets.VBox(\n",
    "    [prompting_type, icl_usage]\n",
    ")\n",
    "\n",
    "title = widgets.HTML(\n",
    "    \"<h1>Cross validation using LLMs</h1>\",\n",
    ")\n",
    "description = widgets.HTML(\n",
    "    \"<div>Set the parameters to select what dataset, model and prompting to use when performing cross validation.</div>\",\n",
    "    layout={\"font-size\": '14px'}\n",
    ")\n",
    "start_cross_validation_button = widgets.Button(\n",
    "    description=\"Start cross validation\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    layout={\"height\": \"40px\", \"width\": \"calc(100% - 4px)\"},\n",
    ")\n",
    "\n",
    "def handle_cross_validation_click(_):\n",
    "    if all_configurations.value:\n",
    "        datasets = [CustomDataset.CLAIMBUSTER.value, CustomDataset.CHECK_THAT.value]\n",
    "        models = [HuggingFaceModel.MISTRAL_7B_INSTRUCT.name, HuggingFaceModel.MIXTRAL_INSTRUCT.name, HuggingFaceModel.LLAMA2_7B_CHAT.name]\n",
    "        prompt_types = [PromptType.STANDARD.value, PromptType.CHAIN_OF_THOUGHT.value]\n",
    "        icl_uses = [ICLUsage.ZERO_SHOT.value, ICLUsage.FEW_SHOT.value]\n",
    "    else:\n",
    "        datasets = [dataset_select.value.value]\n",
    "        models = [model_select.value.name]\n",
    "        prompt_types = [prompting_type.value.value]\n",
    "        icl_uses = [icl_usage.value.value]\n",
    "    for dataset, model, prompt_type, icl_usage in itertools.product(datasets, models, prompt_types, icl_uses):\n",
    "        dataset_folder = os.path.join(\n",
    "            \"results\",\n",
    "            dataset,\n",
    "            model,\n",
    "            prompt_type,\n",
    "            icl_usage,\n",
    "        )\n",
    "        dataset_path = os.path.join(dataset_folder, \"generated_scores.csv\")\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(\"No generated scores found\")\n",
    "            continue\n",
    "        dataset_with_scores = pd.read_csv(dataset_path, index_col=0)\n",
    "        crossval_folder = os.path.join(\n",
    "            \"data\",\n",
    "            dataset,\n",
    "            \"crossval\"\n",
    "        )\n",
    "        label_column = \"Verdict\" if dataset == CustomDataset.CLAIMBUSTER.value else \"check_worthiness\"\n",
    "        print(\"#\" * 50)\n",
    "        print_padded_text(\"Starting cross validation with parameters\")\n",
    "        print_padded_text(f\"Dataset: {dataset}\")\n",
    "        print_padded_text(f\"Model: {model}\")\n",
    "        print_padded_text(f\"Prompting type: {prompt_type}\")\n",
    "        print_padded_text(f\"ICL usage: {icl_usage}\")\n",
    "        print(\"#\" * 50)\n",
    "        result, _ = run_llm_cross_validation(\n",
    "            data=dataset_with_scores, \n",
    "            crossval_folder=crossval_folder,\n",
    "            save_folder=dataset_folder,\n",
    "            label_column=label_column\n",
    "        )\n",
    "        display(result)\n",
    "\n",
    "start_cross_validation_button.on_click(handle_cross_validation_click)\n",
    "\n",
    "accordion = widgets.Accordion([\n",
    "    dataset_select,\n",
    "    model_select,\n",
    "    prompt_use\n",
    "],\n",
    "    titles=[\"Dataset\", \"Model\", \"Prompting type\"],\n",
    ")\n",
    "\n",
    "all_configurations = widgets.Checkbox(\n",
    "    description=\"Run all configurations\",\n",
    "    value=False\n",
    ")\n",
    "\n",
    "def handle_all_configurations_toggled(change):\n",
    "    if change['new'] == True:\n",
    "        accordion.layout.display = \"none\"\n",
    "    else:\n",
    "        accordion.layout.display = \"block\"\n",
    "\n",
    "all_configurations.observe(handle_all_configurations_toggled, names='value')\n",
    "\n",
    "\n",
    "box = widgets.Box(\n",
    "    [title, description, all_configurations, accordion, start_cross_validation_button],\n",
    "    layout=widgets.Layout(\n",
    "        padding= '16px', \n",
    "        display= \"flex\", \n",
    "        flex_flow=\"column\",\n",
    "        align_items=\"stretch\",\n",
    "        border=\"1px solid black\"\n",
    "    )\n",
    ") \n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA finetuning (experiment E4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClaimBuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in [HuggingFaceModel.MISTRAL_7B_INSTRUCT, HuggingFaceModel.LLAMA2_7B_CHAT]:\n",
    "    os.system(\n",
    "        f\"\"\"python3 -m src.lora_finetuning \\\n",
    "                --dataset={CustomDataset.CLAIMBUSTER.value} \\\n",
    "                --model-id={model_id.value} \\\n",
    "                --experiment={Experiment.FINE_TUNING.value}\"\"\"\n",
    "    )\n",
    "    result_path = f\"results/ClaimBuster/{model_id.name}/lora/crossval.csv\"\n",
    "    results = pd.read_csv(result_path, index_col=0)\n",
    "    display(f\"Cross validation resuls for model {model_id.name}\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckThat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in [HuggingFaceModel.MISTRAL_7B_INSTRUCT, HuggingFaceModel.LLAMA2_7B_CHAT]:\n",
    "    os.system(\n",
    "        f\"\"\"python3 -m src.lora_finetuning \\\n",
    "                --dataset={CustomDataset.CHECK_THAT.value} \\\n",
    "                --model-id={model_id.value} \\\n",
    "                --experiment={Experiment.FINE_TUNING.value}\"\"\"\n",
    "    )\n",
    "    result_path = f\"results/CheckThat/{model_id.name}/lora/crossval.csv\"\n",
    "    results = pd.read_csv(result_path, index_col=0)\n",
    "    display(f\"Cross validation resuls for model {model_id.name}\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize all results in one table for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_number(prompt_type: PromptType, icl_usage: ICLUsage) -> str:\n",
    "    if prompt_type == PromptType.CHAIN_OF_THOUGHT:\n",
    "        return \"E3\"\n",
    "    if prompt_type == PromptType.LORA:\n",
    "        return \"E4\"\n",
    "    return \"E1\" if icl_usage == ICLUsage.ZERO_SHOT else \"E2\"\n",
    "\n",
    "for dataset in [CustomDataset.CLAIMBUSTER, CustomDataset.CHECK_THAT]:\n",
    "    all_results = pd.DataFrame(columns=[\"Exp. nr\", \"Model\", \"Prompt type\", \"ICL usage\", \"F1-macro\", \"Accuracy\"])\n",
    "    prompt_types = [PromptType.STANDARD, PromptType.CHAIN_OF_THOUGHT, PromptType.LORA]\n",
    "    icl_uses = [ICLUsage.ZERO_SHOT, ICLUsage.FEW_SHOT]\n",
    "    models = [HuggingFaceModel.MISTRAL_7B_INSTRUCT, HuggingFaceModel.MIXTRAL_INSTRUCT, HuggingFaceModel.LLAMA2_7B_CHAT]\n",
    "    for index, (prompt_type, icl_usage, model) in enumerate(itertools.product(prompt_types, icl_uses, models)):\n",
    "        result_path = os.path.join(\n",
    "            \"results\",\n",
    "            dataset.value,\n",
    "            model.name,\n",
    "            prompt_type.value,\n",
    "            icl_usage.value if prompt_type != PromptType.LORA else '',\n",
    "            \"crossval.csv\"\n",
    "        )\n",
    "        if not os.path.exists(result_path) or prompt_type == PromptType.LORA and icl_usage == ICLUsage.FEW_SHOT:\n",
    "            continue\n",
    "        result = pd.read_csv(result_path, index_col=0)    \n",
    "        accuracy = result.loc[\"Average\", \"accuracy\"]\n",
    "        f1_macro = result.loc[\"Average\", \"macro avg_f1-score\"]\n",
    "        experiment_number = get_experiment_number(prompt_type, icl_usage)\n",
    "        all_results.loc[index] = [\n",
    "            experiment_number,\n",
    "            model.name,\n",
    "            prompt_type.value,\n",
    "            icl_usage.value,\n",
    "            f1_macro,\n",
    "            accuracy,\n",
    "        ]\n",
    "    save_path = os.path.join(\"results\", dataset.value, \"all_results.csv\")\n",
    "    all_results.to_csv(save_path, index=False)\n",
    "    display(dataset.value, all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClaimBuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "predictions = []\n",
    "models = [HuggingFaceModel.MISTRAL_7B_INSTRUCT, HuggingFaceModel.MIXTRAL_INSTRUCT, HuggingFaceModel.LLAMA2_7B_CHAT]\n",
    "prompt_types = [PromptType.STANDARD, PromptType.CHAIN_OF_THOUGHT]\n",
    "icl_usages = [ICLUsage.ZERO_SHOT, ICLUsage.FEW_SHOT]\n",
    "model_names = []\n",
    "claimbuster = load_claimbuster_dataset(\"data/ClaimBuster/datasets\")\n",
    "\n",
    "for model, prompt_type, icl_usage in itertools.product(models, prompt_types, icl_usages):\n",
    "    predictions_path = f\"results/ClaimBuster/{model.name}/{prompt_type.value}/{icl_usage.value}/predictions.csv\"\n",
    "    if os.path.exists(predictions_path):\n",
    "        new_predictions = pd.read_csv(predictions_path, index_col=0)\n",
    "        # Exclude results from LLama 2 model in the final report since they are lackluster\n",
    "        model_name = f\"{model.name} {prompt_type.value} {icl_usage.value}\"\n",
    "        if (model != HuggingFaceModel.LLAMA2_7B_CHAT):\n",
    "            predictions.append(new_predictions) \n",
    "            model_names.append(model_name)\n",
    "        create_confusion_matrix(\n",
    "            claimbuster, \n",
    "            new_predictions.loc[claimbuster.index],\n",
    "            save_path=os.path.join(os.path.dirname(predictions_path), \"confusion-matrix.pdf\")\n",
    "        )\n",
    "# LORA\n",
    "for model in models:\n",
    "    predictions_path = f\"results/ClaimBuster/{model.name}/lora/predictions.csv\"\n",
    "    if os.path.exists(predictions_path):\n",
    "        new_predictions = pd.read_csv(predictions_path, index_col=0)\n",
    "        model_name = f\"{model.name} LORA\"\n",
    "        if (model != HuggingFaceModel.LLAMA2_7B_CHAT):\n",
    "            predictions.append(new_predictions)\n",
    "            model_names.append(model_name)\n",
    "        create_confusion_matrix(\n",
    "            claimbuster, \n",
    "            new_predictions.loc[claimbuster.index][\"prediction\"],\n",
    "            save_path=os.path.join(os.path.dirname(predictions_path), \"confusion-matrix.pdf\")\n",
    "        )\n",
    "mistral_scores = pd.read_csv(f\"results/ClaimBuster/MISTRAL_7B_INSTRUCT/standard/zeroshot/generated_scores.csv\", index_col=0)\n",
    "reasoning = mistral_scores[\"raw_response\"]\n",
    "generate_error_analysis_report(\n",
    "    claimbuster,\n",
    "    predictions=predictions,\n",
    "    model_names=model_names,\n",
    "    folder_path=f\"results/ClaimBuster\",\n",
    "    reasoning=reasoning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckThat 2021 Task 1a Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "predictions = []\n",
    "models = [HuggingFaceModel.MISTRAL_7B_INSTRUCT, HuggingFaceModel.MIXTRAL_INSTRUCT, HuggingFaceModel.LLAMA2_7B_CHAT]\n",
    "prompt_types = [PromptType.STANDARD, PromptType.CHAIN_OF_THOUGHT]\n",
    "icl_usages = [ICLUsage.ZERO_SHOT, ICLUsage.FEW_SHOT]\n",
    "model_names = []\n",
    "checkthat = load_check_that_dataset(\"data/CheckThat\")\n",
    "\n",
    "for model, prompt_type, icl_usage in itertools.product(models, prompt_types, icl_usages):\n",
    "    predictions_path = f\"results/CheckThat/{model.name}/{prompt_type.value}/{icl_usage.value}/predictions.csv\"\n",
    "    if os.path.exists(predictions_path):\n",
    "        new_predictions = pd.read_csv(predictions_path, index_col=0)\n",
    "        model_name = f\"{model.name} {prompt_type.value} {icl_usage.value}\"\n",
    "        # Exclude LLama 2  results since they are lackluster\n",
    "        if model != HuggingFaceModel.LLAMA2_7B_CHAT:\n",
    "            predictions.append(new_predictions) \n",
    "            model_names.append(model_name)\n",
    "        create_confusion_matrix(\n",
    "            checkthat, \n",
    "            new_predictions.loc[checkthat.index],\n",
    "            label_column_name=\"check_worthiness\",\n",
    "            save_path=os.path.join(os.path.dirname(predictions_path), \"confusion-matrix.pdf\")\n",
    "        )\n",
    "# LORA\n",
    "for model in models:\n",
    "    predictions_path = f\"results/CheckThat/{model.name}/lora/predictions.csv\"\n",
    "    if os.path.exists(predictions_path):\n",
    "        new_predictions = pd.read_csv(predictions_path, index_col=0)\n",
    "        model_name = f\"{model.name} LORA\"\n",
    "        if model != HuggingFaceModel.LLAMA2_7B_CHAT:\n",
    "            predictions.append(new_predictions)\n",
    "            model_names.append(model_name)\n",
    "        create_confusion_matrix(\n",
    "            checkthat, \n",
    "            new_predictions.loc[checkthat.index][\"prediction\"],\n",
    "            label_column_name=\"check_worthiness\",\n",
    "            save_path=os.path.join(os.path.dirname(predictions_path), \"confusion-matrix.pdf\")\n",
    "        )\n",
    "\n",
    "mistral_scores = pd.read_csv(f\"results/CheckThat/MISTRAL_7B_INSTRUCT/standard/zeroshot/generated_scores.csv\", index_col=0)\n",
    "reasoning = mistral_scores[\"raw_response\"]\n",
    "generate_error_analysis_report(\n",
    "    checkthat,\n",
    "    predictions=predictions,\n",
    "    model_names=model_names,\n",
    "    folder_path=\"results/CheckThat\",\n",
    "    label_column_name=\"check_worthiness\",\n",
    "    text_column_name=\"tweet_text\",\n",
    "    reasoning=reasoning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relating truthfulness and check-worthiness (experiment E5)\n",
    "\n",
    "Running check-worthiness detection on two datasets used for factual verification by fine-tuning an LLM on the ClaimBuster dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate check-worthiness predictions on factual verifiaction datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [CustomDataset.LIAR, CustomDataset.RAWFC]:\n",
    "    os.system(\n",
    "        f\"\"\"python3 -m src.lora_finetuning \\\n",
    "                --dataset={dataset.value} \\\n",
    "                --model-id={HuggingFaceModel.MISTRAL_7B_INSTRUCT.value} \\\n",
    "                --experiment={Experiment.TRUTH_FULNESS.value}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "liar = pd.read_csv(\"results/LIAR/checkworthiness.csv\", index_col=0)\n",
    "liar.head()\n",
    "label_to_name = {\n",
    "    LIARLabel.PANTS_FIRE: \"Pants on fire\",\n",
    "    LIARLabel.FALSE: \"False\",\n",
    "    LIARLabel.BARELY_TRUE: \"Barely true\",\n",
    "    LIARLabel.HALF_TRUE: \"Half true\",\n",
    "    LIARLabel.MOSTLY_TRUE: \"Mostly true\",\n",
    "    LIARLabel.TRUE: \"True\"\n",
    "}\n",
    "x = [label_to_name[label] for label in LIARLabel]\n",
    "y = [liar[liar[\"label\"] == label.value][\"check_worthiness\"].mean() for label in LIARLabel]\n",
    "file_path = f\"figures/liar/checkworthiness/checkworthiness.pdf\"\n",
    "\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "show_bar_plot(\n",
    "    x, \n",
    "    y, \n",
    "    xlabel=\"Label\", \n",
    "    ylabel=\"Proportion of check-worthy claims\", \n",
    "    y_ticks=[i*0.1 for i in range(11)],\n",
    "    file_path=file_path, \n",
    "    force_save=True,\n",
    "    use_bar_labels=True,\n",
    ")\n",
    "\n",
    "# Look for non-checkworthy claims for each label\n",
    "non_checkworthy_folder = \"results/LIAR/non-checkworthy\"\n",
    "os.makedirs(non_checkworthy_folder, exist_ok=True)\n",
    "checkworthy_folder = \"results/LIAR/checkworthy\"\n",
    "os.makedirs(checkworthy_folder, exist_ok=True)\n",
    "for label in LIARLabel:\n",
    "    non_checkworthy = liar.query(f\"label == {label.value} and check_worthiness == 0\")\n",
    "    non_checkworthy.to_csv(f\"{non_checkworthy_folder}/{label.name}.csv\")\n",
    "    checkworthy = liar.query(f\"label == {label.value} and check_worthiness == 1\")\n",
    "    checkworthy.to_csv(f\"{checkworthy_folder}/{label.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAWFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "from plotting_utils import show_bar_plot\n",
    "rawfc = pd.read_csv(\"results/RAWFC/checkworthiness.csv\", index_col=0)\n",
    "rawfc.head()\n",
    "label_to_name = {\n",
    "        RAWFCLabel.FALSE: \"False\",\n",
    "        RAWFCLabel.HALF_TRUE: \"Half true\",\n",
    "        RAWFCLabel.TRUE: \"True\"\n",
    "    }\n",
    "x = [label_to_name[label] for label in RAWFCLabel]\n",
    "y = [rawfc[rawfc[\"label\"] == label.value][\"check_worthiness\"].mean() for label in RAWFCLabel]\n",
    "file_path = f\"figures/rawfc/checkworthiness/checkworthiness.pdf\"\n",
    "\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "show_bar_plot(\n",
    "    x, \n",
    "    y, \n",
    "    xlabel=\"Label\", \n",
    "    ylabel=\"Proportion of check-worthy claims\", \n",
    "    y_ticks=[i*0.1 for i in range(11)],\n",
    "    file_path=file_path, \n",
    "    force_save=True,\n",
    "    use_bar_labels=True,\n",
    ")\n",
    "\n",
    "# Look for non-checkworthy claims for each label\n",
    "non_checkworthy_folder = \"results/RAWFC/non-checkworthy\"\n",
    "os.makedirs(non_checkworthy_folder, exist_ok=True)\n",
    "checkworthy_folder = \"results/RAWFC/checkworthy\"\n",
    "os.makedirs(checkworthy_folder, exist_ok=True)\n",
    "for label in RAWFCLabel:\n",
    "    non_checkworthy = rawfc.query(f\"label == {label.value} and check_worthiness == 0\")\n",
    "    non_checkworthy.to_csv(f\"{non_checkworthy_folder}/{label.name}.csv\")\n",
    "    checkworthy = rawfc.query(f\"label == {label.value} and check_worthiness == 1\")\n",
    "    checkworthy.to_csv(f\"{checkworthy_folder}/{label.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time evaluation (experiment E6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# General lauyout\n",
    "input_style = dict(\n",
    "    description_width=\"fit-content\"\n",
    ")\n",
    "\n",
    "# Dataset \n",
    "dataset_select = widgets.Dropdown(\n",
    "    options=[(\"ClaimBuster\", CustomDataset.CLAIMBUSTER), (\"CheckThat\", CustomDataset.CHECK_THAT)],\n",
    "    value=CustomDataset.CLAIMBUSTER,\n",
    "    description=\"Dataset:\"\n",
    ")\n",
    "\n",
    "# Model and parameters\n",
    "model_select = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Mistral 7B Instruct\", HuggingFaceModel.MISTRAL_7B_INSTRUCT), \n",
    "        (\"Mixtral Instruct\", HuggingFaceModel.MIXTRAL_INSTRUCT),\n",
    "        (\"LLama 2 7B Chat\", HuggingFaceModel.LLAMA2_7B_CHAT)],\n",
    "    value=HuggingFaceModel.MISTRAL_7B_INSTRUCT,\n",
    "    description=\"Model:\",\n",
    "    style=input_style\n",
    ")\n",
    "max_new_tokens_int_text = widgets.IntText(\n",
    "    value=64,\n",
    "    description=\"Max new tokens:\",\n",
    "    style=input_style\n",
    ")\n",
    "batch_size = widgets.IntText(\n",
    "    value=32,\n",
    "    description=\"Batch size:\",\n",
    "    style=input_style\n",
    ")\n",
    "model_and_parameters = widgets.VBox(\n",
    "    [model_select, max_new_tokens_int_text, batch_size],\n",
    ")\n",
    "\n",
    "# Prompting type\n",
    "prompting_type = widgets.Dropdown(\n",
    "    options=[(\"Standard\", PromptType.STANDARD), (\"Chain-of-Thought\", PromptType.CHAIN_OF_THOUGHT), (\"LORA\", PromptType.LORA)],\n",
    "    value=PromptType.STANDARD,\n",
    "    description=\"Prompting type:\",\n",
    "    style=input_style\n",
    ")\n",
    "icl_usage = widgets.Dropdown(\n",
    "    options=[(\"Zero-shot\", ICLUsage.ZERO_SHOT), (\"Few-shot\", ICLUsage.FEW_SHOT)],\n",
    "    value=ICLUsage.ZERO_SHOT,\n",
    "    description=\"ICL usage:\",\n",
    "    style=input_style\n",
    ")\n",
    "prompt_use = widgets.VBox(\n",
    "    [prompting_type, icl_usage]\n",
    ")\n",
    "\n",
    "accordion = widgets.Accordion([\n",
    "    dataset_select,\n",
    "    model_and_parameters,\n",
    "    prompt_use\n",
    "],\n",
    "    titles=[\"Dataset\", \"Model and parameters\", \"Prompting type\"],\n",
    ")\n",
    "\n",
    "title = widgets.HTML(\n",
    "    \"<h1>Inference time evaluation of LLMs</h1>\",\n",
    ")\n",
    "description = widgets.HTML(\n",
    "    \"<div>Set the parameters to select what dataset, model and prompting to use when generating predictions. If you experience Cuda out of memory issues, please decrease the batch size.</div>\",\n",
    "    layout={\"font-size\": '14px'}\n",
    ")\n",
    "start_generation_button = widgets.Button(\n",
    "    description=\"Start generation\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    layout={\"height\": \"40px\", \"width\": \"calc(100% - 4px)\"},\n",
    ")\n",
    "\n",
    "def handle_generation_click(_):\n",
    "    print(\"#\" * 50)\n",
    "    print_padded_text(\"Starting generation with parameters\")\n",
    "    print_padded_text(f\"Dataset: {dataset_select.value.value}\")\n",
    "    print_padded_text(f\"Model: {model_select.value.name}\")\n",
    "    print_padded_text(f\"Prompting type: {prompting_type.value.value}\")\n",
    "    print_padded_text(f\"ICL usage: {icl_usage.value.value}\")\n",
    "    print(\"#\" * 50)\n",
    "    \n",
    "    if \"src\" in os.getcwd():\n",
    "        os.chdir(\"..\")\n",
    "    if prompting_type.value == PromptType.LORA:\n",
    "        os.system(\n",
    "            f\"\"\"python3 -m src.lora_finetuning \\\n",
    "                    --dataset={CustomDataset.CLAIMBUSTER.value} \\\n",
    "                    --model-id={model_select.value.value} \\\n",
    "                    --experiment={Experiment.INFERENCE_TIME.value}\"\"\"\n",
    "        )\n",
    "    else:\n",
    "        os.system(\n",
    "            f\"\"\"python3 -m src.llm \\\n",
    "                    --experiment={Experiment.INFERENCE_TIME.value} \\\n",
    "                    --dataset={dataset_select.value.value} \\\n",
    "                    --prompt-type={prompting_type.value.value} \\\n",
    "                    --icl_usage={icl_usage.value.value} \\\n",
    "                    --batch-size={batch_size.value} \\\n",
    "                    --max-new-tokens={max_new_tokens_int_text.value} \\\n",
    "                    --model-id={model_select.value.value}\"\"\"\n",
    "        )\n",
    "\n",
    "start_generation_button.on_click(handle_generation_click)\n",
    "\n",
    "\n",
    "box = widgets.Box(\n",
    "    [title, description, accordion, start_generation_button],\n",
    "    layout=widgets.Layout(\n",
    "        padding= '16px', \n",
    "        display= \"flex\", \n",
    "        flex_flow=\"column\",\n",
    "        align_items=\"stretch\",\n",
    "        border=\"1px solid black\"\n",
    "    )\n",
    ") \n",
    "display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
