{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check-worthiness detection using Large Language Models\n",
    "\n",
    "First, the necessary python modules are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/.conda/envs/fakeNews/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-01 14:32:31.042524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-01 14:32:31.042598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-01 14:32:31.044470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-01 14:32:31.055687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 14:32:32.503941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from claimbuster_utils import load_claimbuster_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:48<00:00, 16.08s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config = bnb_config,\n",
    "    # attn_implementation=\"flash_attention_2\", \n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27247</th>\n",
       "      <td>1</td>\n",
       "      <td>We're 9 million jobs short of that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, last year up to this time, we've los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>1</td>\n",
       "      <td>And in November of 1975 I was the first presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19700</th>\n",
       "      <td>1</td>\n",
       "      <td>And what we've done during the Bush administra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you know we don't have a single program spo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Verdict                                               Text\n",
       "sentence_id                                                            \n",
       "27247              1                We're 9 million jobs short of that.\n",
       "10766              1  You know, last year up to this time, we've los...\n",
       "3327               1  And in November of 1975 I was the first presid...\n",
       "19700              1  And what we've done during the Bush administra...\n",
       "12600              1  Do you know we don't have a single program spo..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "  0%|          | 0/9674 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9674 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 31.74 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 30.64 GiB memory in use. Of the allocated memory 27.73 GiB is allocated by PyTorch, and 2.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m non_check_worthy_matcher \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(non-checkworthy)|(not check-worthy)|(non check-worthy)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m responses \u001b[38;5;241m=\u001b[39m pipe(prompts_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    769\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/fakeNews/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 31.74 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 30.64 GiB memory in use. Of the allocated memory 27.73 GiB is allocated by PyTorch, and 2.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "with open(\"../prompts/ClaimBuster/standard/zero-shot.txt\", \"r\") as f:\n",
    "    instruction = f.read()\n",
    "use_contextual = False\n",
    "data = load_claimbuster_dataset(\n",
    "    \"../data/ClaimBuster_Datasets/datasets\",\n",
    "    use_contextual_features=use_contextual,\n",
    "    debate_transcripts_folder=\"../data/ClaimBuster_Datasets/debate_transcripts\",\n",
    ")\n",
    "\n",
    "texts = data[\"Text\"]\n",
    "if use_contextual is False:\n",
    "    prompts = [f\"{instruction} '''{text}'''\" for text in texts]\n",
    "    zeroshot_output = \"../results/ClaimBuster/zeroshot1.csv\"\n",
    "else:\n",
    "    contexts = data[\"previous_sentences\"].tolist()\n",
    "    prompts = [\n",
    "        f\"{instruction} For context, the following senteces were said prior to the one in question: {context} Only evaluate the check-worthiness of the following sentence: '''{text}'''\"\n",
    "        for text, context in zip(texts, contexts)\n",
    "    ]\n",
    "    zeroshot_output = \"../results/ClaimBuster/zeroshot_contextual.csv\"\n",
    "\n",
    "\n",
    "class ProgressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "prompts_data = ProgressDataset(prompts)\n",
    "\n",
    "dataset_with_scores = data.copy()\n",
    "\n",
    "display(data.head())\n",
    "dict_matcher = re.compile(r\"{.*}\")\n",
    "score_matcher = re.compile(r\"([Ss]core[^\\d]*)(\\d+)\")\n",
    "non_check_worthy_matcher = re.compile(\n",
    "    r\"(non-checkworthy)|(not check-worthy)|(non check-worthy)\"\n",
    ")\n",
    "\n",
    "responses = pipe(prompts_data, batch_size=128)\n",
    "for index, result in enumerate(tqdm(responses, total=len(prompts))):\n",
    "    response = result[0][\"generated_text\"].replace(\"\\n\", \"\")\n",
    "    dataset_index = data.index[index]\n",
    "    try:\n",
    "        parsed_json = json.loads(dict_matcher.search(response).group(0))\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = parsed_json[\"score\"]\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = parsed_json[\"reasoning\"]\n",
    "    except (json.decoder.JSONDecodeError, AttributeError) as e:\n",
    "        # Try to find score\n",
    "        score = score_matcher.search(response)\n",
    "        if score is not None:\n",
    "            score = score[2]\n",
    "        else:\n",
    "            score = 0.0 if non_check_worthy_matcher.search(response) else np.nan\n",
    "        dataset_with_scores.loc[dataset_index, \"score\"] = score\n",
    "        dataset_with_scores.loc[dataset_index, \"reasoning\"] = response\n",
    "        continue\n",
    "# Set the following column order: Verdict, score, Text, reasoning, previous_sentences\n",
    "columns =  [\"Verdict\", \"score\", \"Text\", \"reasoning\"]\n",
    "if use_contextual:\n",
    "    columns.append(\"previous_sentences\")\n",
    "dataset_with_scores = dataset_with_scores[columns]\n",
    "dataset_with_scores.to_csv(zeroshot_output, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Verdict                                               Text  \\\n",
      "sentence_id                                                               \n",
      "25315              1  Georgie Anne, we, believe me, supported the Si...   \n",
      "33747              1           I have fought against that as a senator.   \n",
      "15380              1  Six predecessors of mine who served in the Ova...   \n",
      "5730               1  Even FOX News disputes it, and that doesn't ha...   \n",
      "33361              1  He choked and then got into a Twitter war beca...   \n",
      "...              ...                                                ...   \n",
      "8204               0  But let's not put it there; let's put it in te...   \n",
      "32104              0                        Oh, you didn't delete them?   \n",
      "25400              0  We have more at stake in space satellites than...   \n",
      "33659              0  Well, let me translate that, if I can, Chris, ...   \n",
      "19323              0  Now, if these aren't special interests, I've g...   \n",
      "\n",
      "             score                                          reasoning  \n",
      "sentence_id                                                            \n",
      "25315          NaN  This sentence is non-checkworthy as it contain...  \n",
      "33747          NaN   This sentence is non-checkworthy. The reason ...  \n",
      "15380          NaN  This sentence contains a factual claim that si...  \n",
      "5730           NaN  This sentence is non-checkworthy as it is an o...  \n",
      "33361          NaN   The sentence is non check-worthy. The reasoni...  \n",
      "...            ...                                                ...  \n",
      "8204           NaN  This sentence is non-checkworthy as it does no...  \n",
      "32104          NaN   is a non check-worthy sentence. Reasoning: Th...  \n",
      "25400          NaN  This sentence contains a claim that \"we have m...  \n",
      "33659          NaN  This sentence is non check-worthy as it does n...  \n",
      "19323          NaN  This sentence is non-checkworthy as it contain...  \n",
      "\n",
      "[114 rows x 4 columns]\n",
      "{'0': {'precision': 0.9018839487565938, 'recall': 0.8659913169319826, 'f1-score': 0.8835732742709487, 'support': 6910.0}, '1': {'precision': 0.6952945047713064, 'recall': 0.7644717800289436, 'f1-score': 0.7282440117180768, 'support': 2764.0}, 'accuracy': 0.8369857349596858, 'macro avg': {'precision': 0.79858922676395, 'recall': 0.8152315484804631, 'f1-score': 0.8059086429945128, 'support': 9674.0}, 'weighted avg': {'precision': 0.842858393332226, 'recall': 0.8369857349596858, 'f1-score': 0.8391934849701281, 'support': 9674.0}}\n",
      "0.8391934849701281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f41cbc2dc10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwCklEQVR4nO3de3xU9Z3/8ffMJDOTC0mAmAsQrlIV5dZQ0oDdujUV1Icr7v78IWLBVHFLcYvmVysUwf1VJXb9lWXtsk1lQbE3qK61bqV4ieLWEojGKy0GELmIToBgmBAgQ2a+vz8gEyOBzCQh5+Tk9Xw85tHm5MzhM2d3M+/9ns/3+3UZY4wAAABszG11AQAAAO0hsAAAANsjsAAAANsjsAAAANsjsAAAANsjsAAAANsjsAAAANsjsAAAANtLsLqArhCJRPTJJ5+oT58+crlcVpcDAABiYIxRfX29BgwYILf73GMojggsn3zyifLy8qwuAwAAdMC+ffs0aNCgc57jiMDSp08fSac+cFpamsXVAACAWASDQeXl5UW/x8/FEYGl+TFQWloagQUAgB4mlnYOmm4BAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtEVgAAIDtOWLzQ8Bq731cpzd2f2Z1GZYyxigUjijU1PIKG9Ml105K9Ojblw9TZqqvS64HoOchsACd1BSOaOZ/blH9iSarS3G0wJETWjZ9nNVlALAIgQXopD2Hj6n+RJO8CW5NvTTH6nIs43JJiR63fAlueU+/EtztbxnfnmOhsB7/82499+4n+j9TLtLAjKQuqBZAT0NgATpp54GjkqSLsvvo0RnjLa7GmaoD9dr0Ya1W/ekjLblulNXlALAATbdAJzUHlguzUi2uxLn+8esjJElr39irumMhi6sBYIUOBZYVK1Zo6NCh8vv9KigoUGVl5TnPX758uS666CIlJSUpLy9Pd999t06cONGpawJ2QWA5//5mZKZG5abpWCisX1TssbocABaIO7CsW7dOJSUluv/++/XWW29p7NixmjJlig4cONDm+b/+9a+1YMEC3X///dq2bZtWrVqldevW6Yc//GGHrwnYyY4D9ZIILOeTy+XSP359uCTpiU27deJk2OKKAHS3uAPLsmXLNGfOHBUXF2vUqFEqKytTcnKyVq9e3eb5mzZt0uTJk3XzzTdr6NChuuqqqzRjxoxWIyjxXhOwi0jE6MMDDZKkkQSW8+ra0bka1DdJtQ0hPfXmPqvLAdDN4gosoVBIVVVVKioqarmA262ioiJVVFS0+Z5JkyapqqoqGlB27dql9evX65prrunwNRsbGxUMBlu9ACvsrzuu4yfD8nrcGtwv2epyHC3B49acr50aZXnsT7vUFI5YXBGA7hRXYDl06JDC4bCys7NbHc/OzlYgEGjzPTfffLN+9KMf6fLLL1diYqJGjBihK664IvpIqCPXLC0tVXp6evSVl5cXz8cAukxz/8qwzBQleOhhP9/+94Q89Uvxat/h4/rj1rb/PgBwpvM+rXnjxo1aunSp/uM//kMFBQXauXOn5s+frwceeECLFy/u0DUXLlyokpKS6M/BYJDQAkvQcNu9krwezS4cqn99ebse/uMHenlbjdUlnZM/waM7v3Gh8hh9AzotrsCSmZkpj8ejmprWfyRqamqUk9P2glmLFy/Wt771Ld1+++2SpNGjR6uhoUF33HGHFi1a1KFr+nw++Xws0Q3r0XDb/WYVDtHP/+dD7a87rv3vHLe6nHYdqD+hx4snWl0G0OPFFVi8Xq/y8/NVXl6uadOmSZIikYjKy8t15513tvmeY8eOye1uPVTu8Xgkndp7pCPXBOyCEZbu1zfFq7V3fNX2ezedDEf0yAvVerX6oKr2HFb+kH5WlwT0aHE/EiopKdHs2bM1YcIETZw4UcuXL1dDQ4OKi4slSbNmzdLAgQNVWloqSbruuuu0bNkyjR8/PvpIaPHixbruuuuiwaW9awJ2ZIzRjtOBZWQ2gaU7jRmUoTGDMqwuo117ahv0m8p9euSFav1mzlflcnV+qwKgt4o7sEyfPl0HDx7UkiVLFAgENG7cOG3YsCHaNLt3795WIyr33XefXC6X7rvvPu3fv18XXHCBrrvuOj300EMxXxOwo4P1jao/0SS361TTLfBF//SNkfqvqv3avOuw/ryzVpePzLS6JKDHchnTRfu/WygYDCo9PV1HjhxRWlqa1eWgl/jzzkOa+Z9bNCwzRa9+/wqry4FN/d///ose//NujcvL0O++O4lRFuBz4vn+Zh4m0EH0ryAW373iQiUlevTOvjqVb2P1bqCjCCxABzFDCLG4oI9Pt04eKkn6fy9WKxLp8YPagCUILEAHNY+wsCQ/2vOPfzNcfXwJ+iBQr+ff/9TqcoAe6bwvHAc4FY+EEKuMZK9u/9pw/evL27Xk91v1xKbdVpd0Tqm+BN1VNFLjB/e1uhQgisACdMBnDSEdOhqSJI24gMCC9n378qH6xebdOnQ0pKo99l5DRpLe2vuZnvpOoS7OYSID7IHAAnTAzoOnRlcGZiQpxcf/GaF9ffyJ+v2dl2vr/iNWl9Ku//zTLr2x+zPNWlWp/5o7ia0FYAv8pQU6gMdB6IiBGUkamJFkdRnt+uqw/vrfP69QdU29Zq+u1FPfKVT/VLZDgbVougU6YEcNgQXOlZ6cqDXfnqiBGUnadahBxU+8oaONTVaXhV6OERagA5ofCTFDCE6Vk+7Xk7dN1I1lFXrv4yOatuLPXTY65HG75HG7lHD6P90ul7piPb1kb4Lm/e0IDerLIywnIrAAHbCzhjVY4HwjLkjV47d+RTNWbtbOA0ejj0LtLD0pUQuuvtjqMnAeEFiAOB1tbNInR05IIrDA+cbmZejFu/9GW3Yd7pLrGUkRYxSOGDVFjMLhiMJdsJben3ce0isfHFDdsVDnLwZbIrAAcfrw9P+XmZnqU0ay1+JqgPNvUN9kDcq392MWl6RXPjhAr42DEVhgK03hiBpCYavLOKf3T09LpX8FsI/U08sLHLP53w90HIGlHT94+l3lpifpq8P7a/zgDPkTPVaXFJNIxCgUjlhdRrvqTzTp3X11emvvZ6ra85ne+/iIjp/sGX9weBwE2Eey79TfZkZYnIvAcg6HG0L67ZsfS5L+rXyHvAlufXlwhgqG9VdWmk/+BI/8iR75E93yJrjl0rnb3CPG6GQ4olBTRKHT/xkxXbMRWmNTRHtrj2l37THtqW3QnsPHFGqyf2DpqZK9Hk29LMfqMgCc1ryAYwOBxbEILOeQ6HFp6Q2jtXlXrSp21epgfaM27zqszV3UfIZTLsxK1ZcHZ+jLg/sqf0hfDemf0iVTHM8nt+vUdEwA9pBKYHE8Ass59PEn6uaCwbq5YLCMMdp1qEGbd9XqrT11qj9xUieaIjpxMqwTJ8MxjWa4XC55PS55E06NyHg97i770vO4XRrUN1lD+ydrSP8UDctMUd8UbztjPtbzuF095jEbAPtK8Z76Ojva2DMeKSN+BJYYuVwujbggVSMuSNXMgiFWlwMA+BxGWJyPpfkBAD1eyumm2+MnwwpHuqY3EPZCYAEA9Hif3zX9WIhRFicisAAAejxfQktPYAN9LI5EYAEA9Hgul0spXtZicTICCwDAEWi8dTYCCwDAEVg8ztkILAAAR2gOLDwSciYCCwDAEdgA0dkILAAAR0im6dbRCCwAAEeg6dbZCCwAAEeg6dbZCCwAAEdoabqlh8WJCCwAAEdIPb2fECMszkRgAQA4QvSREHsJORKBBQDgCCleelicjMACAHCElqZbeliciMACAHCEFB/rsDgZgQUA4Aip9LA4GoEFAOAIrMPibAQWAIAjpLL5oaMRWAAAjtA8wnLiZEThiLG4GnQ1AgsAwBGaNz+U6GNxIgILAMARfAluJbhdkuhjcSICCwDAEVwuF423DkZgAQA4RiobIDoWgQUA4BgpbIDoWAQWAIBj8EjIuQgsAADHiG6AyCwhxyGwAAAco2U/IXpYnIbAAgBwDB4JOReBBQDgGKkEFscisAAAHCOF/YQcq0OBZcWKFRo6dKj8fr8KCgpUWVl51nOvuOIKuVyuM17XXntt9Jxbb731jN9PnTq1I6UBAHoxRlicKyHeN6xbt04lJSUqKytTQUGBli9frilTpqi6ulpZWVlnnP/MM88oFApFf66trdXYsWN14403tjpv6tSpevzxx6M/+3y+eEsDAPRyzfsJNYRounWauEdYli1bpjlz5qi4uFijRo1SWVmZkpOTtXr16jbP79evn3JycqKvl156ScnJyWcEFp/P1+q8vn37duwTAQB6LZpunSuuwBIKhVRVVaWioqKWC7jdKioqUkVFRUzXWLVqlW666SalpKS0Or5x40ZlZWXpoosu0ty5c1VbW3vWazQ2NioYDLZ6AQDAIyHniiuwHDp0SOFwWNnZ2a2OZ2dnKxAItPv+yspKbd26Vbfffnur41OnTtWTTz6p8vJy/fjHP9Zrr72mq6++WuFw20N6paWlSk9Pj77y8vLi+RgAAIdKYS8hx4q7h6UzVq1apdGjR2vixImtjt90003R/z569GiNGTNGI0aM0MaNG3XllVeecZ2FCxeqpKQk+nMwGCS0AACUyl5CjhXXCEtmZqY8Ho9qampaHa+pqVFOTs4539vQ0KC1a9fqtttua/ffGT58uDIzM7Vz5842f+/z+ZSWltbqBQAAPSzOFVdg8Xq9ys/PV3l5efRYJBJReXm5CgsLz/nep556So2Njbrlllva/Xc+/vhj1dbWKjc3N57yAAC9HHsJOVfcs4RKSkq0cuVKrVmzRtu2bdPcuXPV0NCg4uJiSdKsWbO0cOHCM963atUqTZs2Tf379291/OjRo7rnnnu0efNm7d69W+Xl5br++ut14YUXasqUKR38WACA3qh5hOXEyYiawhGLq0FXiruHZfr06Tp48KCWLFmiQCCgcePGacOGDdFG3L1798rtbp2Dqqur9frrr+vFF18843oej0fvvfee1qxZo7q6Og0YMEBXXXWVHnjgAdZiAQDEpXnzQ+nUWizpSSzo7hQuY4yxuojOCgaDSk9P15EjR+hnAYBebuSi9ToZNtq04BsakJFkdTk4h3i+v4meAABHofHWmQgsAABHaW68ZQNEZyGwAAAcpXm122PsJ+QoBBYAgKMkn268ZYTFWQgsAABHYT8hZyKwAAAcJbp4HIHFUQgsAABHYQNEZyKwAAAchQ0QnYnAAgBwlJYRFgKLkxBYAACOkhKd1kxgcRICCwDAUVK8zY+E6GFxEgILAMBReCTkTAQWAICjsA6LMxFYAACOwgiLMxFYAACOEt2tmaZbRyGwAAAcJbr5IU23jkJgAQA4SrKXzQ+diMACAHCU5hGWxqaImsIRi6tBVyGwAAAcpbmHRWItFichsAAAHMWb4JbXc+rr7SiNt45BYAEAOE4KGyA6DoEFAOA4rMXiPAQWAIDjpHiZ2uw0BBYAgOM0PxJihMU5CCwAAMdJYT8hxyGwAAAcJ5Xl+R2HwAIAcByabp2HwAIAcJxUHgk5DoEFAOA4LeuwMEvIKQgsAADHSfYywuI0BBYAgOPQdOs8BBYAgOO0NN3ySMgpCCwAAMdJZS8hxyGwAAAch4XjnIfAAgBwnBR6WByHwAIAcJyU6CwhelicgsACAHAcNj90HgILAMBxmqc1h5oiOhmOWFwNugKBBQDgOM09LBKNt06R0P4pAAD0LIket7wJboWaIiosfUVuV+eul+xL0PLp4zT5wsyuKRBxI7AAABwpf3BfVeyq1fGTnW+8bQiF9dJfawgsFiKwAAAc6Ze3F2j/Z8c7fZ1fbN6tlX/6SCe6IPig4wgsAABH8rhdGtw/udPXyU7zS1KXjNSg42i6BQDgHJK8p6ZIHw8RWKxEYAEA4BySEk8HFkZYLEVgAQDgHJoDCz0s1iKwAABwDn5vc2BhATorEVgAADgHHgnZA4EFAIBz8CfSdGsHBBYAAM6BHhZ76FBgWbFihYYOHSq/36+CggJVVlae9dwrrrhCLpfrjNe1114bPccYoyVLlig3N1dJSUkqKirSjh07OlIaAABdikdC9hB3YFm3bp1KSkp0//3366233tLYsWM1ZcoUHThwoM3zn3nmGX366afR19atW+XxeHTjjTdGz/mXf/kXPfrooyorK9OWLVuUkpKiKVOm6MSJEx3/ZAAAdAG/99RX5fGTYRljLK6m94o7sCxbtkxz5sxRcXGxRo0apbKyMiUnJ2v16tVtnt+vXz/l5OREXy+99JKSk5OjgcUYo+XLl+u+++7T9ddfrzFjxujJJ5/UJ598omeffbZTHw4AgM5qHmExRmpsYqaQVeIKLKFQSFVVVSoqKmq5gNutoqIiVVRUxHSNVatW6aabblJKSook6aOPPlIgEGh1zfT0dBUUFMR8TQAAzpfmpluJPhYrxbWX0KFDhxQOh5Wdnd3qeHZ2tj744IN2319ZWamtW7dq1apV0WOBQCB6jS9es/l3X9TY2KjGxsboz8FgMObPAABAPBI9biV6XDoZNjp+MqwMqwvqpbp1ltCqVas0evRoTZw4sVPXKS0tVXp6evSVl5fXRRUCAHAmpjZbL67AkpmZKY/Ho5qamlbHa2pqlJOTc873NjQ0aO3atbrttttaHW9+XzzXXLhwoY4cORJ97du3L56PAQBAXJgpZL24AovX61V+fr7Ky8ujxyKRiMrLy1VYWHjO9z711FNqbGzULbfc0ur4sGHDlJOT0+qawWBQW7ZsOes1fT6f0tLSWr0AADhfkrysxWK1uHpYJKmkpESzZ8/WhAkTNHHiRC1fvlwNDQ0qLi6WJM2aNUsDBw5UaWlpq/etWrVK06ZNU//+/Vsdd7lcuuuuu/Tggw9q5MiRGjZsmBYvXqwBAwZo2rRpHf9kAAB0kZbF45glZJW4A8v06dN18OBBLVmyRIFAQOPGjdOGDRuiTbN79+6V29164Ka6ulqvv/66XnzxxTav+YMf/EANDQ264447VFdXp8svv1wbNmyQ3+/vwEcCAKBr0cNiPZdxwCo4wWBQ6enpOnLkCI+HAABd7qbHKrR512H9dMZ4XTd2gNXlOEY839/sJQQAQDtourUegQUAgHbQdGs9AgsAAO2gh8V6BBYAANrBIyHrEVgAAGgHgcV6BBYAANoR7WHhkZBlCCwAALTDzwiL5QgsAAC0o+WRECvdWoXAAgBAO5jWbD0CCwAA7WjZS4jAYhUCCwAA7fAnnvq6ZB0W6xBYAABoB0231iOwAADQDtZhsR6BBQCAdrAOi/UILAAAtIMRFusRWAAAaAc9LNYjsAAA0I6WdVgiikSMxdX0TgQWAADa0fxISJIam1jt1goEFgAA2uH/XGDhsZA1CCwAALTD43bJm3B68TgCiyUILAAAxIDl+a1FYAEAIAbRqc2sxWIJAgsAADFo3k+IERZrEFgAAIgBa7FYi8ACAEAMmtdi4ZGQNQgsAADEgOX5rUVgAQAgBswSshaBBQCAGPh5JGQpAgsAADFoeSTE0vxWILAAABADelisRWABACAGLTs2E1isQGABACAGfppuLUVgAQAgBizNby0CCwAAMWhemp8eFmsQWAAAiAHrsFiLwAIAQAyiS/MTWCxBYAEAIAZ+elgsRWABACAGLBxnLQILAAAxYB0WaxFYAACIAdOarUVgAQAgBn6W5rcUgQUAgBgwS8haBBYAAGLQ/Ego1BRROGIsrqb3IbAAABCD5sAiSY1NjLJ0NwILAAAx8CW0fGXSeNv9CCwAAMTA7XZFQwt9LN2PwAIAQIxYi8U6BBYAAGLUshYLq912NwILAAAxSmItFssQWAAAiBGLx1mHwAIAQIyii8cxS6jbdSiwrFixQkOHDpXf71dBQYEqKyvPeX5dXZ3mzZun3Nxc+Xw+felLX9L69eujv//nf/5nuVyuVq+LL764I6UBAHDeND8Soum2+yXE+4Z169appKREZWVlKigo0PLlyzVlyhRVV1crKyvrjPNDoZC++c1vKisrS08//bQGDhyoPXv2KCMjo9V5l156qV5++eWWwhLiLg0AgPOKR0LWiTsVLFu2THPmzFFxcbEkqaysTM8//7xWr16tBQsWnHH+6tWrdfjwYW3atEmJiYmSpKFDh55ZSEKCcnJy4i0HAIBuwyMh68T1SCgUCqmqqkpFRUUtF3C7VVRUpIqKijbf89xzz6mwsFDz5s1Tdna2LrvsMi1dulThcOv/Ye/YsUMDBgzQ8OHDNXPmTO3du/esdTQ2NioYDLZ6AQBwviUlnvraPMHS/N0ursBy6NAhhcNhZWdntzqenZ2tQCDQ5nt27dqlp59+WuFwWOvXr9fixYv1k5/8RA8++GD0nIKCAj3xxBPasGGDfvazn+mjjz7S1772NdXX17d5zdLSUqWnp0dfeXl58XwMAAA6JNrDwghLtzvvjSKRSERZWVl67LHH5PF4lJ+fr/379+uRRx7R/fffL0m6+uqro+ePGTNGBQUFGjJkiH7729/qtttuO+OaCxcuVElJSfTnYDBIaAEAnHf0sFgnrsCSmZkpj8ejmpqaVsdramrO2n+Sm5urxMREeTwtu1xecsklCgQCCoVC8nq9Z7wnIyNDX/rSl7Rz5842r+nz+eTz+eIpHQCATiOwWCeuR0Jer1f5+fkqLy+PHotEIiovL1dhYWGb75k8ebJ27typSKRlGePt27crNze3zbAiSUePHtWHH36o3NzceMoDAOC8amm6ZWn+7hb3OiwlJSVauXKl1qxZo23btmnu3LlqaGiIzhqaNWuWFi5cGD1/7ty5Onz4sObPn6/t27fr+eef19KlSzVv3rzoOd///vf12muvaffu3dq0aZNuuOEGeTwezZgxows+IgAAXYN1WKwTdw/L9OnTdfDgQS1ZskSBQEDjxo3Thg0boo24e/fuldvdkoPy8vL0wgsv6O6779aYMWM0cOBAzZ8/X/fee2/0nI8//lgzZsxQbW2tLrjgAl1++eXavHmzLrjggi74iAAAdA32ErKOyxhjrC6is4LBoNLT03XkyBGlpaVZXQ4AwKGee/cTfe83b6tweH/95o6vWl1OjxfP9zd7CQEAECNGWKxDYAEAIEb0sFiHwAIAQIySvKe+Nhlh6X4EFgAAYhRdh4WVbrsdgQUAgBjxSMg6BBYAAGLUvHDciZMsHNfdCCwAAMTIn3AqsITCETWFCS3dicACAECMmkdYJOlEE4GlOxFYAACIkS+h5WuTxtvuRWABACBGLpeLxluLEFgAAIhDdMdmAku3IrAAABCHJNZisQSBBQCAOPgTWe3WCgQWAADiwCMhaxBYAACIQ7TplkdC3YrAAgBAHJr3EzrRRGDpTgQWAADi0NJ0y8Jx3YnAAgBAHKI7NtPD0q0ILAAAxIGF46xBYAEAIA7RWUI03XYrAgsAAHHgkZA1CCwAAMQhicBiCQILAABxSPKe+upkHZbuRWABACAOjLBYg8ACAEAc6GGxBoEFAIA4MEvIGgQWAADiwDos1iCwAAAQh5bAwtL83YnAAgBAHPxeelisQGABACAO/gQCixUILAAAxKG56ZZ1WLoXgQUAgDiwDos1CCwAAMShObA0RYxOhmm87S4EFgAA4uD3tnx1MsrSfRKsLgAAgJ7E63HL7ZIiRtp1sEH9U7ydup4/0aML+vi6qDrnIrAAABAHl8ulpESPGkJhTVvx5y655tIbRuvmgsFdci2n4pEQAABxuuHLA5WU6On0y5tw6mt47Rt7Lf5E9ucyxhiri+isYDCo9PR0HTlyRGlpaVaXAwBATA7WN2ri0pdljPTnBd/QwIwkq0vqVvF8fzPCAgCARS7o49NXhvSTJL34l4DF1dgbgQUAAAtddWm2JOkFAss5EVgAALDQlEtzJEmVHx3W4YaQxdXYF4EFAAAL5fVL1qUD0hQx0st/rbG6HNsisAAAYLHmURYeC50dgQUAAItNvexUYPnTjkM62thkcTX2RGABAMBiI7NSNSwzRaFwRBurD1hdji0RWAAAsJjL5frcbCH6WNpCYAEAwAamnu5jefWDA2psYlPFLyKwAABgA2MHZSg7zaejjU3atLPW6nJsh8ACAIANuN0uXTXq1CjLhq3MFvoidmsGAMAmpl6Wo19s3qP173+q4yc7/1go2evRvL+9UHn9krugOmt1KLCsWLFCjzzyiAKBgMaOHauf/vSnmjhx4lnPr6ur06JFi/TMM8/o8OHDGjJkiJYvX65rrrmmw9cEAMBpJg7rp8xUrw4dDem5dz/pkmv28Sdo0bWjuuRaVoo7sKxbt04lJSUqKytTQUGBli9frilTpqi6ulpZWVlnnB8KhfTNb35TWVlZevrppzVw4EDt2bNHGRkZHb4mAABOlOhx6xe3FWjTh53vYan4sFYvb6vRZ8dOdkFl1nMZY0w8bygoKNBXvvIV/fu//7skKRKJKC8vT//0T/+kBQsWnHF+WVmZHnnkEX3wwQdKTEzskmt+UTzbUwMA0Bv8YvMeLX52q6ZemqOyb+VbXU6b4vn+jqvpNhQKqaqqSkVFRS0XcLtVVFSkioqKNt/z3HPPqbCwUPPmzVN2drYuu+wyLV26VOFwuMPXbGxsVDAYbPUCAAAtUn0eSVJDyBkr58YVWA4dOqRwOKzs7OxWx7OzsxUItN3RvGvXLj399NMKh8Nav369Fi9erJ/85Cd68MEHO3zN0tJSpaenR195eXnxfAwAABwv1XfqqUb9iV4YWDoiEokoKytLjz32mPLz8zV9+nQtWrRIZWVlHb7mwoULdeTIkehr3759XVgxAAA9X8rpERan7E0UV9NtZmamPB6PampaLxtcU1OjnJycNt+Tm5urxMREeTye6LFLLrlEgUBAoVCoQ9f0+Xzy+XzxlA4AQK/S5/QIS4NDAktcIyxer1f5+fkqLy+PHotEIiovL1dhYWGb75k8ebJ27typSCQSPbZ9+3bl5ubK6/V26JoAAODcoiMsvfWRUElJiVauXKk1a9Zo27Ztmjt3rhoaGlRcXCxJmjVrlhYuXBg9f+7cuTp8+LDmz5+v7du36/nnn9fSpUs1b968mK8JAADik+o/9RDlaKhJcU4ItqW412GZPn26Dh48qCVLligQCGjcuHHasGFDtGl27969crtbclBeXp5eeOEF3X333RozZowGDhyo+fPn69577435mgAAID6pvlNf8cZIx0Jhpfh69uL2ca/DYkeswwIAQGvGGI344XpFjFT5wyuVlea3uqQznLd1WAAAQM/gcrmioyr1Dmi8JbAAAOBQfU4HFifMFCKwAADgUM0jLE6YKURgAQDAoaIzhRhhAQAAdtU8U4jAAgAAbCuVHhYAAGB3zBICAAC2xwgLAACwvVRmCQEAALtrniXEIyEAAGBbPBICAAC2x7RmAABgey2BJWxxJZ1HYAEAwKFaluY/aXElnUdgAQDAofr4m3tYGGEBAAA2lUIPCwAAsLvoLKFQkyIRY3E1nUNgAQDAoZoDizHSsZM9+7EQgQUAAIfyJ7rlcbsk9fy1WAgsAAA4lMvlUorXI0mq7+HL8xNYAABwsD7+REmMsAAAABtL8Z0aYenpM4UILAAAOJhTlucnsAAA4GAtq90SWAAAgE1FV7sNEVgAAIBNpXhPBRZmCQEAANtK9dPDAgAAbC66PD+BBQAA2FUqTbcAAMDunLJjM4EFAAAH60MPCwAAsLvmWUL0sAAAANtqniVUT2ABAAB2xSwhAABge8wSAgAAtpcaXZo/rEjEWFxNxxFYAABwsOYRFqln7ydEYAEAwMF8CW4luF2SpIbGsMXVdByBBQAAB3O5XJ9bPO6kxdV0HIEFAACHizbeMsICAADsygkzhQgsAAA4XKqfR0IAAMDmUngkBAAA7K5P9JEQIywAAMCmUnweSacWj+upCCwAADhcqi9RklRP0y0AALCr1OYRlh68ASKBBQAAh2uZJURgAQAANtUyS4jAAgAAbIqF4wAAgO01B5Zet1vzihUrNHToUPn9fhUUFKiysvKs5z7xxBNyuVytXn6/v9U5t9566xnnTJ06tSOlAQCAL3DCCEtCvG9Yt26dSkpKVFZWpoKCAi1fvlxTpkxRdXW1srKy2nxPWlqaqquroz+7XK4zzpk6daoef/zx6M8+ny/e0gAAQBt6ZQ/LsmXLNGfOHBUXF2vUqFEqKytTcnKyVq9efdb3uFwu5eTkRF/Z2dlnnOPz+Vqd07dv33hLAwAAbejT22YJhUIhVVVVqaioqOUCbreKiopUUVFx1vcdPXpUQ4YMUV5enq6//nr95S9/OeOcjRs3KisrSxdddJHmzp2r2tras16vsbFRwWCw1QsAALSteYTlWCiscMRYXE3HxBVYDh06pHA4fMYISXZ2tgKBQJvvueiii7R69Wr9/ve/1y9/+UtFIhFNmjRJH3/8cfScqVOn6sknn1R5ebl+/OMf67XXXtPVV1+tcLjtJYRLS0uVnp4efeXl5cXzMQAA6FWae1ikntt4G3cPS7wKCwtVWFgY/XnSpEm65JJL9POf/1wPPPCAJOmmm26K/n706NEaM2aMRowYoY0bN+rKK68845oLFy5USUlJ9OdgMEhoAQDgLHwJbiW4XWqKGDU0NinNn2h1SXGLa4QlMzNTHo9HNTU1rY7X1NQoJycnpmskJiZq/Pjx2rlz51nPGT58uDIzM896js/nU1paWqsXAABom8vlalnttofOFIorsHi9XuXn56u8vDx6LBKJqLy8vNUoyrmEw2G9//77ys3NPes5H3/8sWpra895DgAAiF2K91Rgqe+hjbdxzxIqKSnRypUrtWbNGm3btk1z585VQ0ODiouLJUmzZs3SwoULo+f/6Ec/0osvvqhdu3bprbfe0i233KI9e/bo9ttvl3SqIfeee+7R5s2btXv3bpWXl+v666/XhRdeqClTpnTRxwQAoHdrninUUzdAjLuHZfr06Tp48KCWLFmiQCCgcePGacOGDdFG3L1798rtbslBn332mebMmaNAIKC+ffsqPz9fmzZt0qhRoyRJHo9H7733ntasWaO6ujoNGDBAV111lR544AHWYgEAoIv09MXjXMaYnjm/6XOCwaDS09N15MgR+lkAAGjD7NWVem37QT3yv8boxgn2mKgSz/c3ewkBANALpPbwxeMILAAA9AKp3p7dw0JgAQCgF2geYek1s4QAAEDP07w8PyMsAADAtvr08FlCBBYAAHqB5hGWo41t79NndwQWAAB6gZZZQictrqRjCCwAAPQCqT6PJKmBERYAAGBXqb5TOzSzDgsAALCtlNMjLAQWAABgW32aR1iYJQQAAOyqeYTl+MmwmsIRi6uJH4EFAIBeoHmWkCQ1hHpe421C+6cAAICezpfgUaLHpZNho6XPb1Py6RGXWCW4XVp07ajzVF0M/75l/zIAAOhWWX382l93XOve3Bf3e70JbgILAAA4/x6dMV6vfFDTofd63NZ2kRBYAADoJfKH9FX+kL5Wl9EhNN0CAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbI7AAAADbc8RuzcYYSVIwGLS4EgAAEKvm7+3m7/FzcURgqa+vlyTl5eVZXAkAAIhXfX290tPTz3mOy8QSa2wuEonok08+UZ8+feRyuTp8nWAwqLy8PO3bt09paWldWCHawv3uXtzv7sX97n7c8+7VFffbGKP6+noNGDBAbve5u1QcMcLidrs1aNCgLrteWloa/8vejbjf3Yv73b24392Pe969Onu/2xtZaUbTLQAAsD0CCwAAsD0Cy+f4fD7df//98vl8VpfSK3C/uxf3u3txv7sf97x7dff9dkTTLQAAcDZGWAAAgO0RWAAAgO0RWAAAgO0RWAAAgO0RWD5nxYoVGjp0qPx+vwoKClRZWWl1SY5QWlqqr3zlK+rTp4+ysrI0bdo0VVdXtzrnxIkTmjdvnvr376/U1FT9wz/8g2pqaiyq2DkefvhhuVwu3XXXXdFj3Ouut3//ft1yyy3q37+/kpKSNHr0aL355pvR3xtjtGTJEuXm5iopKUlFRUXasWOHhRX3XOFwWIsXL9awYcOUlJSkESNG6IEHHmi1Fw33u+P+53/+R9ddd50GDBggl8ulZ599ttXvY7m3hw8f1syZM5WWlqaMjAzddtttOnr0aOeLMzDGGLN27Vrj9XrN6tWrzV/+8hczZ84ck5GRYWpqaqwurcebMmWKefzxx83WrVvNO++8Y6655hozePBgc/To0eg53/nOd0xeXp4pLy83b775pvnqV79qJk2aZGHVPV9lZaUZOnSoGTNmjJk/f370OPe6ax0+fNgMGTLE3HrrrWbLli1m165d5oUXXjA7d+6MnvPwww+b9PR08+yzz5p3333X/N3f/Z0ZNmyYOX78uIWV90wPPfSQ6d+/v/nDH/5gPvroI/PUU0+Z1NRU82//9m/Rc7jfHbd+/XqzaNEi88wzzxhJ5ne/+12r38dyb6dOnWrGjh1rNm/ebP70pz+ZCy+80MyYMaPTtRFYTps4caKZN29e9OdwOGwGDBhgSktLLazKmQ4cOGAkmddee80YY0xdXZ1JTEw0Tz31VPScbdu2GUmmoqLCqjJ7tPr6ejNy5Ejz0ksvma9//evRwMK97nr33nuvufzyy8/6+0gkYnJycswjjzwSPVZXV2d8Pp/5zW9+0x0lOsq1115rvv3tb7c69vd///dm5syZxhjud1f6YmCJ5d7+9a9/NZLMG2+8ET3nj3/8o3G5XGb//v2dqodHQpJCoZCqqqpUVFQUPeZ2u1VUVKSKigoLK3OmI0eOSJL69esnSaqqqtLJkydb3f+LL75YgwcP5v530Lx583Tttde2uqcS9/p8eO655zRhwgTdeOONysrK0vjx47Vy5cro7z/66CMFAoFW9zw9PV0FBQXc8w6YNGmSysvLtX37dknSu+++q9dff11XX321JO73+RTLva2oqFBGRoYmTJgQPaeoqEhut1tbtmzp1L/viM0PO+vQoUMKh8PKzs5udTw7O1sffPCBRVU5UyQS0V133aXJkyfrsssukyQFAgF5vV5lZGS0Ojc7O1uBQMCCKnu2tWvX6q233tIbb7xxxu+4111v165d+tnPfqaSkhL98Ic/1BtvvKHvfe978nq9mj17dvS+tvX3hXsevwULFigYDOriiy+Wx+NROBzWQw89pJkzZ0oS9/s8iuXeBgIBZWVltfp9QkKC+vXr1+n7T2BBt5o3b562bt2q119/3epSHGnfvn2aP3++XnrpJfn9fqvL6RUikYgmTJigpUuXSpLGjx+vrVu3qqysTLNnz7a4Ouf57W9/q1/96lf69a9/rUsvvVTvvPOO7rrrLg0YMID77XA8EpKUmZkpj8dzxkyJmpoa5eTkWFSV89x55536wx/+oFdffVWDBg2KHs/JyVEoFFJdXV2r87n/8auqqtKBAwf05S9/WQkJCUpISNBrr72mRx99VAkJCcrOzuZed7Hc3FyNGjWq1bFLLrlEe/fulaTofeXvS9e45557tGDBAt10000aPXq0vvWtb+nuu+9WaWmpJO73+RTLvc3JydGBAwda/b6pqUmHDx/u9P0nsEjyer3Kz89XeXl59FgkElF5ebkKCwstrMwZjDG688479bvf/U6vvPKKhg0b1ur3+fn5SkxMbHX/q6urtXfvXu5/nK688kq9//77euedd6KvCRMmaObMmdH/zr3uWpMnTz5jmv727ds1ZMgQSdKwYcOUk5PT6p4Hg0Ft2bKFe94Bx44dk9vd+qvL4/EoEolI4n6fT7Hc28LCQtXV1amqqip6ziuvvKJIJKKCgoLOFdCpll0HWbt2rfH5fOaJJ54wf/3rX80dd9xhMjIyTCAQsLq0Hm/u3LkmPT3dbNy40Xz66afR17Fjx6LnfOc73zGDBw82r7zyinnzzTdNYWGhKSwstLBq5/j8LCFjuNddrbKy0iQkJJiHHnrI7Nixw/zqV78yycnJ5pe//GX0nIcffthkZGSY3//+9+a9994z119/PdNsO2j27Nlm4MCB0WnNzzzzjMnMzDQ/+MEPoudwvzuuvr7evP322+btt982ksyyZcvM22+/bfbs2WOMie3eTp061YwfP95s2bLFvP7662bkyJFMa+5qP/3pT83gwYON1+s1EydONJs3b7a6JEeQ1Obr8ccfj55z/Phx893vftf07dvXJCcnmxtuuMF8+umn1hXtIF8MLNzrrvff//3f5rLLLjM+n89cfPHF5rHHHmv1+0gkYhYvXmyys7ONz+czV155pamurrao2p4tGAya+fPnm8GDBxu/32+GDx9uFi1aZBobG6PncL877tVXX23z7/Xs2bONMbHd29raWjNjxgyTmppq0tLSTHFxsamvr+90bS5jPrc8IAAAgA3RwwIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGyPwAIAAGzv/wNvjPJGt4poFAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the number of empty scores\n",
    "dataset_with_scores = pd.read_csv(\"../results/ClaimBuster/zeroshot1.csv\", index_col=0)\n",
    "print(dataset_with_scores[dataset_with_scores[\"score\"].isna()])\n",
    "y_gold = dataset_with_scores[\"Verdict\"].values\n",
    "reports = []\n",
    "for threshold in range(1, 100):\n",
    "    y_pred = dataset_with_scores[\"score\"].map(lambda x: 1 if x >= threshold else 0).values\n",
    "    reports.append(classification_report(y_gold, y_pred, output_dict=True))\n",
    "# Print highest f1-score\n",
    "print(max(reports, key=lambda report: report[\"macro avg\"][\"f1-score\"]))\n",
    "print(max([report[\"weighted avg\"][\"f1-score\"] for report in reports]))\n",
    "plt.plot(range(1, 100), [report[\"macro avg\"][\"f1-score\"] for report in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
