{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check-worthiness detection using Large Language Models\n",
    "\n",
    "First, the necessary python modules are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 15:05:03.960963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 15:05:03.961044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 15:05:03.962882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-07 15:05:03.974677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 15:05:06.088859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from claimbuster_utils import load_claimbuster_dataset\n",
    "from checkthat_utils import load_check_that_dataset\n",
    "import pandas as pd\n",
    "from llm import load_huggingface_model, HuggingFaceModel, run_llm_cross_validation, generate_llm_predictions, ICLUsage, PromptType\n",
    "from result_analysis import generate_error_analysis_report, print_padded_text\n",
    "from dataset_utils import generate_cross_validation_datasets, Dataset\n",
    "import ipywidgets as widgets\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cross Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "claimbuster = load_claimbuster_dataset(\"../data/ClaimBuster/datasets\")\n",
    "clambuster_datasets = generate_cross_validation_datasets(\n",
    "    data=claimbuster, \n",
    "    folder_path=\"../data/ClaimBuster/crossval\"\n",
    ")\n",
    "\n",
    "checkthat = load_check_that_dataset(\"../data/CheckThat\")\n",
    "checkthat_datasets = generate_cross_validation_datasets(\n",
    "    data=checkthat, \n",
    "    label_column=\"check_worthiness\",\n",
    "    folder_path=\"../data/CheckThat/crossval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [03:01<00:00,  9.54s/it]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "model_id = HuggingFaceModel.MIXTRAL_INSTRUCT\n",
    "pipe = load_huggingface_model(model_id, max_new_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions\n",
    "\n",
    "Using ipywidgets to select which model, dataset, and other parameters to generate LLM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de26d79dc2c484f90333d115ef50f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='<h1>Generation of predictions using LLMs</h1>'), HTML(value='<div>Set the parameters…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "#      Starting generation with parameters       #\n",
      "#              Dataset: ClaimBuster              #\n",
      "#           Model: MISTRAL_7B_INSTRUCT           #\n",
      "#            Prompting type: standard            #\n",
      "#               ICL usage: fewshot               #\n",
      "##################################################\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1683e1e26643b49a1a20a8ad83f76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_new_tokens=64\n",
      "Generating predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd53eee8ba024c759e698cefe7fb016c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/fake-news-detection/Fake-news-detection/src/llm.py:154: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '30' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dataset_with_scores.loc[dataset_index, \"score\"] = score\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# General lauyout\n",
    "input_style = dict(\n",
    "    description_width=\"fit-content\"\n",
    ")\n",
    "\n",
    "# Dataset \n",
    "dataset_select = widgets.Dropdown(\n",
    "    options=[(\"ClaimBuster\", Dataset.CLAIMBUSTER), (\"CheckThat\", Dataset.CHECK_THAT)],\n",
    "    value=Dataset.CLAIMBUSTER,\n",
    "    description=\"Dataset:\"\n",
    ")\n",
    "\n",
    "# Model and parameters\n",
    "model_select = widgets.Dropdown(\n",
    "    options=[(\"Mistral 7B Instruct\", HuggingFaceModel.MISTRAL_7B_INSTRUCT), (\"Mixtral Instruct\", HuggingFaceModel.MIXTRAL_INSTRUCT)],\n",
    "    value=HuggingFaceModel.MISTRAL_7B_INSTRUCT,\n",
    "    description=\"Model:\",\n",
    "    style=input_style\n",
    ")\n",
    "max_new_tokens_int_text = widgets.IntText(\n",
    "    value=64,\n",
    "    description=\"Max new tokens:\",\n",
    "    style=input_style\n",
    ")\n",
    "batch_size = widgets.IntText(\n",
    "    value=128,\n",
    "    description=\"Batch size:\",\n",
    "    style=input_style\n",
    ")\n",
    "model_and_parameters = widgets.VBox(\n",
    "    [model_select, max_new_tokens_int_text, batch_size],\n",
    ")\n",
    "\n",
    "# Prompting type\n",
    "prompting_type = widgets.Dropdown(\n",
    "    options=[(\"Standard\", PromptType.STANDARD), (\"Chain-of-Thought\", PromptType.CHAIN_OF_THOUGHT)],\n",
    "    value=PromptType.STANDARD,\n",
    "    description=\"Prompting type:\",\n",
    "    style=input_style\n",
    ")\n",
    "icl_usage = widgets.Dropdown(\n",
    "    options=[(\"Zero-shot\", ICLUsage.ZERO_SHOT), (\"Few-shot\", ICLUsage.FEW_SHOT)],\n",
    "    value=ICLUsage.ZERO_SHOT,\n",
    "    description=\"ICL usage:\",\n",
    "    style=input_style\n",
    ")\n",
    "prompt_use = widgets.VBox(\n",
    "    [prompting_type, icl_usage]\n",
    ")\n",
    "\n",
    "accordion = widgets.Accordion([\n",
    "    dataset_select,\n",
    "    model_and_parameters,\n",
    "    prompt_use\n",
    "],\n",
    "    titles=[\"Dataset\", \"Model and parameters\", \"Prompting type\"],\n",
    ")\n",
    "\n",
    "title = widgets.HTML(\n",
    "    \"<h1>Generation of predictions using LLMs</h1>\",\n",
    ")\n",
    "description = widgets.HTML(\n",
    "    \"<div>Set the parameters to select what dataset, model and prompting to use when generating predictions. If you experience Cuda out of memory issues, please decrease the batch size.</div>\",\n",
    "    layout={\"font-size\": '14px'}\n",
    ")\n",
    "start_generation_button = widgets.Button(\n",
    "    description=\"Start generation\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    layout={\"height\": \"40px\", \"width\": \"calc(100% - 4px)\"},\n",
    ")\n",
    "\n",
    "def handle_generation_click(_):\n",
    "    if dataset_select.value == Dataset.CLAIMBUSTER:\n",
    "        dataset = load_claimbuster_dataset(\"../data/ClaimBuster/datasets\")\n",
    "        label_column = \"Verdict\"\n",
    "        text_column = \"Text\"\n",
    "    else:\n",
    "        dataset = load_check_that_dataset(\"../data/CheckThat\")\n",
    "        label_column = \"check_worthiness\"\n",
    "        text_column = \"tweet_text\"\n",
    "\n",
    "    instruction_path = os.path.join(\n",
    "        \"../prompts\",\n",
    "        dataset_select.value.value,\n",
    "        prompting_type.value.value,\n",
    "        icl_usage.value.value,\n",
    "        \"instruction.txt\"\n",
    "    )\n",
    "    print(\"#\" * 50)\n",
    "    print_padded_text(\"Starting generation with parameters\")\n",
    "    print_padded_text(f\"Dataset: {dataset_select.value.value}\")\n",
    "    print_padded_text(f\"Model: {model_select.value.name}\")\n",
    "    print_padded_text(f\"Prompting type: {prompting_type.value.value}\")\n",
    "    print_padded_text(f\"ICL usage: {icl_usage.value.value}\")\n",
    "    print(\"#\" * 50)\n",
    "    if not os.path.exists(instruction_path):\n",
    "        print(\"No instruction found, exiting...\")\n",
    "        return\n",
    "    with open(instruction_path, \"r\") as f:\n",
    "        instruction = f.read().replace(\"\\n\", \"\")\n",
    "    prompts = [ f\"[INST]{instruction} '''{text}'''[/INST]\" for text in dataset[text_column]]\n",
    "    print(\"Loading model...\")\n",
    "    pipe = load_huggingface_model(\n",
    "        model_id=model_select.value, \n",
    "        max_new_tokens=max_new_tokens_int_text.value\n",
    "    )\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    save_path = os.path.join(\n",
    "        \"../results\",\n",
    "        dataset_select.value.value,\n",
    "        model_select.value.name,\n",
    "        prompting_type.value.value,\n",
    "        icl_usage.value.value,\n",
    "        \"generated_scores.csv\"\n",
    "    )\n",
    "    generate_llm_predictions(\n",
    "        data=dataset,\n",
    "        prompts=prompts,\n",
    "        pipe=pipe,\n",
    "        batch_size=batch_size.value,\n",
    "        label_column=label_column,\n",
    "        text_column=text_column,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "\n",
    "start_generation_button.on_click(handle_generation_click)\n",
    "\n",
    "\n",
    "box = widgets.Box(\n",
    "    [title, description, accordion, start_generation_button],\n",
    "    layout=widgets.Layout(\n",
    "        padding= '16px', \n",
    "        display= \"flex\", \n",
    "        flex_flow=\"column\",\n",
    "        align_items=\"stretch\",\n",
    "        border=\"1px solid black\"\n",
    "    )\n",
    ") \n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Using ipywidgets to select what models to run cross validation on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268be91bd68647229f6ed8046838d738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='<h1>Cross validation using LLMs</h1>'), HTML(value='<div>Set the parameters to selec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "#   Starting cross validation with parameters    #\n",
      "#              Dataset: ClaimBuster              #\n",
      "#           Model: MISTRAL_7B_INSTRUCT           #\n",
      "#            Prompting type: standard            #\n",
      "#               ICL usage: fewshot               #\n",
      "##################################################\n",
      "self.threshold=31\n",
      "self.threshold=8\n",
      "self.threshold=31\n",
      "self.threshold=8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>0_precision</th>\n",
       "      <th>0_recall</th>\n",
       "      <th>0_f1-score</th>\n",
       "      <th>1_precision</th>\n",
       "      <th>1_recall</th>\n",
       "      <th>1_f1-score</th>\n",
       "      <th>macro avg_precision</th>\n",
       "      <th>macro avg_recall</th>\n",
       "      <th>macro avg_f1-score</th>\n",
       "      <th>weighted avg_precision</th>\n",
       "      <th>weighted avg_recall</th>\n",
       "      <th>weighted avg_f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809012</td>\n",
       "      <td>0.824949</td>\n",
       "      <td>0.929977</td>\n",
       "      <td>0.874320</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.506512</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.784024</td>\n",
       "      <td>0.718245</td>\n",
       "      <td>0.738365</td>\n",
       "      <td>0.801568</td>\n",
       "      <td>0.809012</td>\n",
       "      <td>0.796647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.746589</td>\n",
       "      <td>0.912657</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.800909</td>\n",
       "      <td>0.536517</td>\n",
       "      <td>0.829233</td>\n",
       "      <td>0.651507</td>\n",
       "      <td>0.724587</td>\n",
       "      <td>0.771387</td>\n",
       "      <td>0.726208</td>\n",
       "      <td>0.805211</td>\n",
       "      <td>0.746589</td>\n",
       "      <td>0.758232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.797767</td>\n",
       "      <td>0.822732</td>\n",
       "      <td>0.913723</td>\n",
       "      <td>0.865844</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.507959</td>\n",
       "      <td>0.589421</td>\n",
       "      <td>0.762366</td>\n",
       "      <td>0.710841</td>\n",
       "      <td>0.727632</td>\n",
       "      <td>0.788230</td>\n",
       "      <td>0.797767</td>\n",
       "      <td>0.786849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.757651</td>\n",
       "      <td>0.930566</td>\n",
       "      <td>0.713955</td>\n",
       "      <td>0.807995</td>\n",
       "      <td>0.548033</td>\n",
       "      <td>0.866860</td>\n",
       "      <td>0.671525</td>\n",
       "      <td>0.739299</td>\n",
       "      <td>0.790407</td>\n",
       "      <td>0.739760</td>\n",
       "      <td>0.821248</td>\n",
       "      <td>0.757651</td>\n",
       "      <td>0.768995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.872726</td>\n",
       "      <td>0.817799</td>\n",
       "      <td>0.837267</td>\n",
       "      <td>0.632412</td>\n",
       "      <td>0.677641</td>\n",
       "      <td>0.628715</td>\n",
       "      <td>0.752569</td>\n",
       "      <td>0.747720</td>\n",
       "      <td>0.732991</td>\n",
       "      <td>0.804064</td>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.777681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy  0_precision  0_recall  0_f1-score  1_precision  1_recall  \\\n",
       "0        0.809012     0.824949  0.929977    0.874320     0.743100  0.506512   \n",
       "1        0.746589     0.912657  0.713542    0.800909     0.536517  0.829233   \n",
       "2        0.797767     0.822732  0.913723    0.865844     0.702000  0.507959   \n",
       "3        0.757651     0.930566  0.713955    0.807995     0.548033  0.866860   \n",
       "Average  0.777755     0.872726  0.817799    0.837267     0.632412  0.677641   \n",
       "\n",
       "         1_f1-score  macro avg_precision  macro avg_recall  \\\n",
       "0          0.602410             0.784024          0.718245   \n",
       "1          0.651507             0.724587          0.771387   \n",
       "2          0.589421             0.762366          0.710841   \n",
       "3          0.671525             0.739299          0.790407   \n",
       "Average    0.628715             0.752569          0.747720   \n",
       "\n",
       "         macro avg_f1-score  weighted avg_precision  weighted avg_recall  \\\n",
       "0                  0.738365                0.801568             0.809012   \n",
       "1                  0.726208                0.805211             0.746589   \n",
       "2                  0.727632                0.788230             0.797767   \n",
       "3                  0.739760                0.821248             0.757651   \n",
       "Average            0.732991                0.804064             0.777755   \n",
       "\n",
       "         weighted avg_f1-score  \n",
       "0                     0.796647  \n",
       "1                     0.758232  \n",
       "2                     0.786849  \n",
       "3                     0.768995  \n",
       "Average               0.777681  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "input_style = dict(\n",
    "    description_width=\"fit-content\"\n",
    ")\n",
    "\n",
    "dataset_select = widgets.Dropdown(\n",
    "    options=[(\"ClaimBuster\", Dataset.CLAIMBUSTER), (\"CheckThat\", Dataset.CHECK_THAT)],\n",
    "    value=Dataset.CLAIMBUSTER,\n",
    "    description=\"Dataset:\"\n",
    ")\n",
    "\n",
    "model_select = widgets.Dropdown(\n",
    "    options=[(\"Mistral 7B Instruct\", HuggingFaceModel.MISTRAL_7B_INSTRUCT), (\"Mixtral Instruct\", HuggingFaceModel.MIXTRAL_INSTRUCT)],\n",
    "    value=HuggingFaceModel.MISTRAL_7B_INSTRUCT,\n",
    "    description=\"Model:\",\n",
    "    style=input_style\n",
    ")\n",
    "\n",
    "prompting_type = widgets.Dropdown(\n",
    "    options=[(\"Standard\", PromptType.STANDARD), (\"Chain-of-Thought\", PromptType.CHAIN_OF_THOUGHT)],\n",
    "    value=PromptType.STANDARD,\n",
    "    description=\"Prompting type:\",\n",
    "    style=input_style\n",
    ")\n",
    "icl_usage = widgets.Dropdown(\n",
    "    options=[(\"Zero-shot\", ICLUsage.ZERO_SHOT), (\"Few-shot\", ICLUsage.FEW_SHOT)],\n",
    "    value=ICLUsage.ZERO_SHOT,\n",
    "    description=\"ICL usage:\",\n",
    "    style=input_style\n",
    ")\n",
    "prompt_use = widgets.VBox(\n",
    "    [prompting_type, icl_usage]\n",
    ")\n",
    "\n",
    "title = widgets.HTML(\n",
    "    \"<h1>Cross validation using LLMs</h1>\",\n",
    ")\n",
    "description = widgets.HTML(\n",
    "    \"<div>Set the parameters to select what dataset, model and prompting to use when performing cross validation.</div>\",\n",
    "    layout={\"font-size\": '14px'}\n",
    ")\n",
    "start_cross_validation_button = widgets.Button(\n",
    "    description=\"Start cross validation\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    layout={\"height\": \"40px\", \"width\": \"calc(100% - 4px)\"},\n",
    ")\n",
    "\n",
    "def handle_cross_validation_click(_):\n",
    "    dataset_folder = os.path.join(\n",
    "        \"../results\",\n",
    "        dataset_select.value.value,\n",
    "        model_select.value.name,\n",
    "        prompting_type.value.value,\n",
    "        icl_usage.value.value,\n",
    "    )\n",
    "    dataset_path = os.path.join(dataset_folder, \"generated_scores.csv\")\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"No generated scores found\")\n",
    "        return\n",
    "    dataset_with_scores = pd.read_csv(dataset_path, index_col=0)\n",
    "    crossval_folder = os.path.join(\n",
    "        \"../data\",\n",
    "        dataset_select.value.value,\n",
    "        \"crossval\"\n",
    "    )\n",
    "    label_column = \"Verdict\" if dataset_select.value == Dataset.CLAIMBUSTER else \"check_worthiness\"\n",
    "    print(\"#\" * 50)\n",
    "    print_padded_text(\"Starting cross validation with parameters\")\n",
    "    print_padded_text(f\"Dataset: {dataset_select.value.value}\")\n",
    "    print_padded_text(f\"Model: {model_select.value.name}\")\n",
    "    print_padded_text(f\"Prompting type: {prompting_type.value.value}\")\n",
    "    print_padded_text(f\"ICL usage: {icl_usage.value.value}\")\n",
    "    print(\"#\" * 50)\n",
    "    result, _ = run_llm_cross_validation(\n",
    "        data=dataset_with_scores, \n",
    "        crossval_folder=crossval_folder,\n",
    "        save_folder=dataset_folder,\n",
    "        label_column=label_column\n",
    "    )\n",
    "    display(result)\n",
    "\n",
    "start_cross_validation_button.on_click(handle_cross_validation_click)\n",
    "\n",
    "accordion = widgets.Accordion([\n",
    "    dataset_select,\n",
    "    model_select,\n",
    "    prompt_use\n",
    "],\n",
    "    titles=[\"Dataset\", \"Model\", \"Prompting type\"],\n",
    ")\n",
    "\n",
    "box = widgets.Box(\n",
    "    [title, description, accordion, start_cross_validation_button],\n",
    "    layout=widgets.Layout(\n",
    "        padding= '16px', \n",
    "        display= \"flex\", \n",
    "        flex_flow=\"column\",\n",
    "        align_items=\"stretch\",\n",
    "        border=\"1px solid black\"\n",
    "    )\n",
    ") \n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using contextual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_claimbuster_dataset(\n",
    "    \"../data/ClaimBuster/datasets\",\n",
    "    use_contextual_features=True,\n",
    "    debate_transcripts_folder=\"../data/ClaimBuster/debate_transcripts\",\n",
    ")[:10]\n",
    "\n",
    "contexts = data[\"previous_sentences\"].tolist()\n",
    "prompts = [\n",
    "    f\"{instruction} For context, the following senteces were said prior to the one in question: {context} Only evaluate the check-worthiness of the following sentence: '''{text}'''\"\n",
    "    for text, context in zip(texts, contexts)\n",
    "]\n",
    "zeroshot_output = \"../results/ClaimBuster/{model_id.name}/zeroshot/zeroshot_contextual_preds.csv\"\n",
    "\n",
    "generate_llm_predictions(\n",
    "    data=data,\n",
    "    pipe=pipe, \n",
    "    prompts=prompts, \n",
    "    save_path=zeroshot_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27247</th>\n",
       "      <td>1</td>\n",
       "      <td>We're 9 million jobs short of that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, last year up to this time, we've los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>1</td>\n",
       "      <td>And in November of 1975 I was the first presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19700</th>\n",
       "      <td>1</td>\n",
       "      <td>And what we've done during the Bush administra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you know we don't have a single program spo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Verdict                                               Text\n",
       "sentence_id                                                            \n",
       "27247              1                We're 9 million jobs short of that.\n",
       "10766              1  You know, last year up to this time, we've los...\n",
       "3327               1  And in November of 1975 I was the first presid...\n",
       "19700              1  And what we've done during the Bush administra...\n",
       "12600              1  Do you know we don't have a single program spo..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "#              MISTRAL_7B_INSTRUCT               #\n",
      "#              False positives: 913              #\n",
      "#              False negatives: 726              #\n",
      "##################################################\n",
      "#                MIXTRAL_INSTRUCT                #\n",
      "#              False positives: 913              #\n",
      "#              False negatives: 726              #\n",
      "##################################################\n",
      "#                      LORA                      #\n",
      "#              False positives: 366              #\n",
      "#              False negatives: 406              #\n",
      "##################################################\n",
      "#                     Total                      #\n",
      "#             False positives: 1109              #\n",
      "#              False negatives: 860              #\n",
      "#        Overlapping false positives: 170        #\n",
      "#        Overlapping false negatives: 272        #\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "mistral_predictins = pd.read_csv(f\"../results/ClaimBuster/{HuggingFaceModel.MISTRAL_7B_INSTRUCT.name}/zeroshot/predictions.csv\", index_col=0)\n",
    "mixtral_predictions = pd.read_csv(f\"../results/ClaimBuster/{HuggingFaceModel.MIXTRAL_INSTRUCT.name}/zeroshot/predictions.csv\", index_col=0)\n",
    "lora_predictions = pd.read_csv(f\"../results/ClaimBuster/{HuggingFaceModel.MISTRAL_7B_INSTRUCT.name}/lora/predictions.csv\", index_col=0)\n",
    "predictions = [mistral_predictins, mistral_predictins, lora_predictions]\n",
    "model_names = [HuggingFaceModel.MISTRAL_7B_INSTRUCT.name, HuggingFaceModel.MIXTRAL_INSTRUCT.name, \"LORA\"]\n",
    "display(claimbuster.head())\n",
    "generate_error_analysis_report(\n",
    "    claimbuster,\n",
    "    predictions=predictions,\n",
    "    model_names=model_names,\n",
    "    folder_path=f\"../results/ClaimBuster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheckThat 2021 Task 1a Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "#              MISTRAL_7B_INSTRUCT               #\n",
      "#              False positives: 315              #\n",
      "#              False negatives: 102              #\n",
      "##################################################\n",
      "#                MIXTRAL_INSTRUCT                #\n",
      "#              False positives: 161              #\n",
      "#              False negatives: 131              #\n",
      "##################################################\n",
      "#                      LORA                      #\n",
      "#              False positives: 115              #\n",
      "#              False negatives: 77               #\n",
      "##################################################\n",
      "#                     Total                      #\n",
      "#              False positives: 390              #\n",
      "#              False negatives: 182              #\n",
      "#        Overlapping false positives: 54         #\n",
      "#        Overlapping false negatives: 32         #\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "folder_path = f\"../results/CheckThat\"\n",
    "mistral_predictions = pd.read_csv(f\"{folder_path}/{HuggingFaceModel.MISTRAL_7B_INSTRUCT.name}/zeroshot/predictions.csv\", index_col=0)\n",
    "mixtral_predictions = pd.read_csv(f\"{folder_path}/{HuggingFaceModel.MIXTRAL_INSTRUCT.name}/zeroshot/predictions.csv\", index_col=0)\n",
    "lora_predictions = pd.read_csv(f\"{folder_path}/{HuggingFaceModel.MISTRAL_7B_INSTRUCT.name}/lora/predictions.csv\", index_col=0)\n",
    "results = [mistral_predictions, mixtral_predictions, lora_predictions]\n",
    "model_names = [HuggingFaceModel.MISTRAL_7B_INSTRUCT.name, HuggingFaceModel.MIXTRAL_INSTRUCT.name, \"LORA\"]\n",
    "generate_error_analysis_report(\n",
    "    checkthat,\n",
    "    predictions=results,\n",
    "    model_names=model_names,\n",
    "    folder_path=folder_path,\n",
    "    label_column_name=\"check_worthiness\",\n",
    "    text_column_name=\"tweet_text\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
