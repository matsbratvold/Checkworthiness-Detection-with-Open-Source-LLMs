{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIsVQyq8hb_c"
   },
   "source": [
    "# Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYZlrsxTW3_T"
   },
   "source": [
    "Links to Google Colab containing other claim spotting models can be found below:\n",
    "\n",
    "\n",
    "*   [BiLSTM](https://colab.research.google.com/github/idirlab/claimspotter/blob/master/bidirectional_lstm/bilstm-notebook.ipynb)\n",
    "*   [SVM](https://colab.research.google.com/github/idirlab/claimspotter/blob/master/svm/svm-notebook.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZZ8f3i3hfYI"
   },
   "source": [
    "    Copyright (C) 2020 IDIR Lab - UT Arlington\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License v3 as published by\n",
    "    the Free Software Foundation.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "    Contact Information:\n",
    "    See: https://idir.uta.edu/cli.html\n",
    "\n",
    "    Chengkai Li\n",
    "    Box 19015\n",
    "    Arlington, TX 76019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERl7MzA-yumh"
   },
   "source": [
    "# Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0O8wRXsLe_8"
   },
   "outputs": [],
   "source": [
    "!git clone --branch develop_new_models https://github.com/idirlab/claimspotter.git\n",
    "!pip3 install transformers==3.5.1\n",
    "!pip3 install emoji\n",
    "\n",
    "%cd claimspotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWYfq1zZfyc5"
   },
   "source": [
    "## Model output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ilPe1se5gaiO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 18:31:54.071889: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-19 18:31:54.088080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-19 18:31:54.463826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-19 18:31:55.169650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-19 18:32:10.652525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices()) \n",
    "print(tf.config.list_physical_devices('GPU')) \n",
    "\n",
    "os.environ[\"BB\"] = \"./output/bb/\"\n",
    "os.environ[\"BBA\"] = \"./output/bba/\"\n",
    "os.environ[\"DB\"] = \"./output/db/\"\n",
    "os.environ[\"DBA\"] = \"./output/dba/\"\n",
    "os.environ[\"RB\"] = \"./output/rb/\"\n",
    "os.environ[\"RBA\"] = \"./output/rba/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_PdF4zdy0re"
   },
   "source": [
    "# BERT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chgaME0gzUT9"
   },
   "source": [
    "### Standard BERT model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5DpLL03WJ-v"
   },
   "source": [
    "Demonstration of the BERT claimspotting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/fake-news-detection/Fake-news-detection/claimbuster-spotter-master\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:absl:{'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'stderrthreshold': 'info', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': False, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/tmp/absl_testing', 'test_randomize_ordering_seed': '', 'xml_output_file': '', 'cs_gpu': ['0'], 'cs_ner_spacy': False, 'cs_model_dir': './output/bb/', 'cs_model_ckpt': 'claimspotter.ckpt', 'cs_data_dir': './data', 'cs_kfold_data_file': 'kfold_25ncs.json', 'cs_reg_train_file': 'train.json', 'cs_reg_test_file': 'test.json', 'cs_tb_dir': './tb_logs', 'cs_data_file_encoding': None, 'cs_use_clef_data': False, 'cs_refresh_data': False, 'cs_max_len': 200, 'cs_remove_stopwords': False, 'cs_sklearn_oversample': False, 'cs_weight_classes_loss': False, 'cs_custom_preprc': True, 'cs_random_state': 59, 'cs_num_classes': 2, 'cs_alt_two_class_combo': False, 'cs_stat_print_interval': 1, 'cs_model_save_interval': 1, 'cs_k_fold': 4, 'cs_restore_and_continue': False, 'cs_batch_size_reg': 24, 'cs_batch_size_adv': 12, 'cs_train_steps': 5, 'cs_lr': 5e-05, 'cs_l2_reg_coeff': 0.0, 'cs_adv_train': False, 'cs_perturb_id': 6, 'cs_adv_type': 0, 'cs_lambda': 0.25, 'cs_lambda_eps': 10.0, 'cs_combine_reg_adv_loss': True, 'cs_perturb_norm_length_range': [0.5, 5.0], 'cs_tfm_type': 'bert-base-uncased', 'cs_hidden_size': 768, 'cs_pool_strat': 'first', 'cs_tfm_ft_embed': False, 'cs_tfm_ft_pooler': True, 'cs_tfm_ft_enc_layers': 2, 'cs_kp_tfm_attn': 0.8, 'cs_kp_tfm_hidden': 0.8, 'cs_cls_hidden': 0, 'cs_kp_cls': 0.7, 'cs_custom_activation': True, 'cs_ca_r': 0.4, 'cs_model_loc': './data/bert-base-uncased_pretrain', 'cs_raw_kfold_data_loc': './data/two_class/kfold_25ncs.json', 'cs_raw_data_loc': './data/two_class/train.json', 'cs_raw_dj_eval_loc': './data/two_class/test.json', 'cs_raw_clef_train_loc': './data/clef20/task5/train.tsv', 'cs_raw_clef_test_loc': './data/clef20/task5/test-gold.tsv', 'cs_prc_data_loc': './data/all_data_bert-base-uncased.pickle', 'cs_prc_clef_loc': './data/all_clef_data.pickle'}\n",
      "INFO:absl:Loading dataset\n",
      "INFO:absl:Restoring data from ./data/all_data_bert-base-uncased.pickle\n",
      "INFO:absl:Class weights computed to be [0.7  1.75]\n",
      "INFO:absl:9674 total cross-validation examples\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #1: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    4    6 ... 9668 9671 9672]\n",
      "INFO:absl:[   1    2    3 ... 9669 9670 9673]\n",
      "INFO:absl:[4636, 1893]\n",
      "INFO:absl:[1728, 691]\n",
      "2024-02-19 18:32:55.595076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-02-19 18:32:55.617552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.74GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-02-19 18:32:55.618354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-19 18:32:55.621681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-19 18:32:55.624517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-19 18:32:55.625579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-19 18:32:55.628670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-19 18:32:55.630180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-19 18:32:55.635747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-19 18:32:55.636330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-02-19 18:32:55.636865: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-02-19 18:32:55.643099: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199885000 Hz\n",
      "2024-02-19 18:32:55.643355: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2a78000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-19 18:32:55.643398: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-02-19 18:32:55.960326: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5561b07e84a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-19 18:32:55.960362: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2024-02-19 18:32:55.979086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.74GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-02-19 18:32:55.979159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-19 18:32:55.979190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-19 18:32:55.979216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-19 18:32:55.979243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-19 18:32:55.979269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-19 18:32:55.979295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-19 18:32:55.979321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-19 18:32:55.979751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-02-19 18:32:56.005609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-19 18:32:56.008857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-02-19 18:32:56.008882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-02-19 18:32:56.008897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-02-19 18:32:56.041475: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-02-19 18:32:56.041533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30259 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Warming up...\n",
      "2024-02-19 18:33:16.144764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'dense/kernel:0', 'dense/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:49<00:00,  6.65it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:53<00:00,  5.06it/s]\n",
      "INFO:absl:Epoch   1 Loss:  0.5241 Acc: 75.4786% Dev Loss:  0.2625 Dev Acc:  0.8857 F1-Mac:  0.8476 F1-Wei:  0.8860(67.077 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.68it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3193 Acc: 86.4451% Dev Loss:  0.2504 Dev Acc:  0.9022 F1-Mac:  0.8706 F1-Wei:  0.9028(47.253 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.68it/s]\n",
      "INFO:absl:Epoch   3 Loss:   0.289 Acc: 88.1452% Dev Loss:  0.2557 Dev Acc:  0.9008 F1-Mac:  0.8734 F1-Wei:  0.9031(47.264 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.67it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2657 Acc: 89.0795% Dev Loss:  0.2642 Dev Acc:  0.8981 F1-Mac:  0.8672 F1-Wei:  0.8995(47.391 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.67it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2451 Acc: 89.8913% Dev Loss:  0.2309 Dev Acc:  0.9105 F1-Mac:  0.8793 F1-Wei:  0.9102(47.330 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_01_005\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_01_005/claimspotter.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.21it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #1 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #2: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9671 9672 9673]\n",
      "INFO:absl:[   7    9   14 ... 9652 9658 9662]\n",
      "INFO:absl:[4642, 1887]\n",
      "INFO:absl:[1728, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:48<00:00,  6.66it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:53<00:00,  5.14it/s]\n",
      "INFO:absl:Epoch   1 Loss:  0.5234 Acc: 74.7434% Dev Loss:  0.2665 Dev Acc:  0.8898 F1-Mac:  0.8549 F1-Wei:  0.8896(66.109 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:41<00:00,  6.65it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3042 Acc: 87.1037% Dev Loss:  0.2528 Dev Acc:  0.8981 F1-Mac:  0.8561 F1-Wei:  0.8940(47.471 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:41<00:00,  6.65it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2765 Acc: 88.5741% Dev Loss:  0.2242 Dev Acc:  0.9063 F1-Mac:  0.8720 F1-Wei:  0.9043(47.574 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:41<00:00,  6.65it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2555 Acc: 89.0948% Dev Loss:  0.2371 Dev Acc:  0.9036 F1-Mac:  0.8769 F1-Wei:  0.9048(47.515 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:41<00:00,  6.64it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2377 Acc: 90.0291% Dev Loss:  0.2470 Dev Acc:  0.8994 F1-Mac:  0.8547 F1-Wei:  0.8940(47.528 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_02_004\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_02_004/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.08it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #2 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #3: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9670 9671 9673]\n",
      "INFO:absl:[   4   11   13 ... 9665 9667 9672]\n",
      "INFO:absl:[4678, 1852]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:48<00:00,  6.63it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [01:00<00:00,  4.51it/s]\n",
      "INFO:absl:Epoch   1 Loss:  0.5286 Acc: 74.8698% Dev Loss:  0.3142 Dev Acc:  0.8457 F1-Mac:  0.8282 F1-Wei:  0.8497(82.347 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.68it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3199 Acc: 86.9066% Dev Loss:  0.2361 Dev Acc:  0.8939 F1-Mac:  0.8740 F1-Wei:  0.8936(47.259 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.67it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2819 Acc: 88.2848% Dev Loss:  0.2376 Dev Acc:  0.9022 F1-Mac:  0.8844 F1-Wei:  0.9021(47.318 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.68it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2593 Acc: 89.4181% Dev Loss:  0.2204 Dev Acc:  0.9091 F1-Mac:  0.8921 F1-Wei:  0.9089(47.263 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.68it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2414 Acc: 90.0000% Dev Loss:  0.2087 Dev Acc:  0.9174 F1-Mac:  0.9019 F1-Wei:  0.9171(47.249 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_03_005\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_03_005/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.21it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #3 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #4: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   1    2    3 ... 9670 9672 9673]\n",
      "INFO:absl:[   0    6   10 ... 9666 9668 9671]\n",
      "INFO:absl:[4648, 1882]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:48<00:00,  6.65it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:53<00:00,  5.15it/s]\n",
      "INFO:absl:Epoch   1 Loss:   0.511 Acc: 76.0031% Dev Loss:  0.3075 Dev Acc:  0.8691 F1-Mac:  0.8364 F1-Wei:  0.8711(66.591 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.67it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3139 Acc: 87.0597% Dev Loss:  0.2791 Dev Acc:  0.8884 F1-Mac:  0.8613 F1-Wei:  0.8904(47.305 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.69it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2855 Acc: 88.2083% Dev Loss:  0.2516 Dev Acc:  0.9050 F1-Mac:  0.8792 F1-Wei:  0.9056(47.245 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.68it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2569 Acc: 89.5865% Dev Loss:  0.2738 Dev Acc:  0.9105 F1-Mac:  0.8819 F1-Wei:  0.9094(47.266 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.69it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2423 Acc: 90.1072% Dev Loss:  0.2878 Dev Acc:  0.9022 F1-Mac:  0.8769 F1-Wei:  0.9034(47.257 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_04_004\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_04_004/claimspotter.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.20it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #4 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:[0. 0. 0. ... 0. 0. 1.]\n",
      "INFO:absl:[[-0.85584873  1.58546054]\n",
      " [ 2.6082375  -4.00612545]\n",
      " [ 0.70703739 -0.67633802]\n",
      " ...\n",
      " [ 3.79127598 -2.82402778]\n",
      " [ 1.97317696 -1.31182241]\n",
      " [-3.24532199  2.83707714]]\n",
      "INFO:absl:Final stats | F1-Mac: 0.8795 F1-Wei: 0.9019 nDCG: 0.9875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9287    0.9347    0.9317      6910\n",
      "         CFS     0.8341    0.8205    0.8273      2764\n",
      "\n",
      "    accuracy                         0.9021      9674\n",
      "   macro avg     0.8814    0.8776    0.8795      9674\n",
      "weighted avg     0.9017    0.9021    0.9019      9674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /cluster/home/matssbra/fake-news-detection/Fake-news-detection/claimbuster-spotter-master\n",
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$BB \\\n",
    "    --cs_adv_train=False \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73EtHQT9uMCP"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$BB \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RaCy2vEzbTL"
   },
   "source": [
    "### Adversarial BERT model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pmBLZuntzTc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:absl:{'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'stderrthreshold': 'info', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': False, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/tmp/absl_testing', 'test_randomize_ordering_seed': '', 'xml_output_file': '', 'cs_gpu': ['0'], 'cs_ner_spacy': False, 'cs_model_dir': './output/bba/', 'cs_model_ckpt': 'claimspotter.ckpt', 'cs_data_dir': './data', 'cs_kfold_data_file': 'kfold_25ncs.json', 'cs_reg_train_file': 'train.json', 'cs_reg_test_file': 'test.json', 'cs_tb_dir': './tb_logs', 'cs_data_file_encoding': None, 'cs_use_clef_data': False, 'cs_refresh_data': False, 'cs_max_len': 200, 'cs_remove_stopwords': False, 'cs_sklearn_oversample': False, 'cs_weight_classes_loss': False, 'cs_custom_preprc': True, 'cs_random_state': 59, 'cs_num_classes': 2, 'cs_alt_two_class_combo': False, 'cs_stat_print_interval': 1, 'cs_model_save_interval': 1, 'cs_k_fold': 4, 'cs_restore_and_continue': False, 'cs_batch_size_reg': 24, 'cs_batch_size_adv': 12, 'cs_train_steps': 10, 'cs_lr': 5e-05, 'cs_l2_reg_coeff': 0.0, 'cs_adv_train': True, 'cs_perturb_id': 6, 'cs_adv_type': 0, 'cs_lambda': 0.1, 'cs_lambda_eps': 10.0, 'cs_combine_reg_adv_loss': True, 'cs_perturb_norm_length_range': [0.5, 5.0], 'cs_tfm_type': 'bert-base-uncased', 'cs_hidden_size': 768, 'cs_pool_strat': 'first', 'cs_tfm_ft_embed': False, 'cs_tfm_ft_pooler': True, 'cs_tfm_ft_enc_layers': 2, 'cs_kp_tfm_attn': 0.8, 'cs_kp_tfm_hidden': 0.8, 'cs_cls_hidden': 0, 'cs_kp_cls': 0.7, 'cs_custom_activation': True, 'cs_ca_r': 0.4, 'cs_model_loc': './data/bert-base-uncased_pretrain', 'cs_raw_kfold_data_loc': './data/two_class/kfold_25ncs.json', 'cs_raw_data_loc': './data/two_class/train.json', 'cs_raw_dj_eval_loc': './data/two_class/test.json', 'cs_raw_clef_train_loc': './data/clef20/task5/train.tsv', 'cs_raw_clef_test_loc': './data/clef20/task5/test-gold.tsv', 'cs_prc_data_loc': './data/all_data_bert-base-uncased.pickle', 'cs_prc_clef_loc': './data/all_clef_data.pickle'}\n",
      "INFO:absl:Loading dataset\n",
      "INFO:absl:Restoring data from ./data/all_data_bert-base-uncased.pickle\n",
      "INFO:absl:Class weights computed to be [0.7  1.75]\n",
      "INFO:absl:9674 total cross-validation examples\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #1: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    4    6 ... 9668 9671 9672]\n",
      "INFO:absl:[   1    2    3 ... 9669 9670 9673]\n",
      "INFO:absl:[4636, 1893]\n",
      "INFO:absl:[1728, 691]\n",
      "2024-02-19 19:01:18.480663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-02-19 19:01:18.501666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.74GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-02-19 19:01:18.502385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-19 19:01:18.505782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-19 19:01:18.508700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-19 19:01:18.509534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-19 19:01:18.515033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-19 19:01:18.516574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-19 19:01:18.522127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-19 19:01:18.522813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-02-19 19:01:18.523345: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-02-19 19:01:18.529541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199885000 Hz\n",
      "2024-02-19 19:01:18.529811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb398000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-19 19:01:18.529833: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-02-19 19:01:18.798011: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56224a05f2a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-19 19:01:18.798046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2024-02-19 19:01:18.798416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.74GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-02-19 19:01:18.798494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-19 19:01:18.798525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-19 19:01:18.798550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-19 19:01:18.798576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-19 19:01:18.798601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-19 19:01:18.798627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-19 19:01:18.798652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-19 19:01:18.799057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-02-19 19:01:18.799108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-19 19:01:18.799476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-02-19 19:01:18.799496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-02-19 19:01:18.799508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-02-19 19:01:18.799997: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-02-19 19:01:18.800047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30259 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Warming up...\n",
      "2024-02-19 19:01:20.724268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'dense/kernel:0', 'dense/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:13<00:00,  2.82it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 77.8373% Dev Loss:  0.2726 Dev Acc:  0.8884 F1-Mac:  0.8534 F1-Wei:  0.8895(207.422 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 87.2109% Dev Loss:  0.2495 Dev Acc:  0.9146 F1-Mac:  0.8819 F1-Wei:  0.9132(165.195 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -168.9 Acc: 88.6659% Dev Loss:  0.2231 Dev Acc:  0.9022 F1-Mac:  0.8656 F1-Wei:  0.9009(164.994 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.7 Acc: 89.3246% Dev Loss:  0.2114 Dev Acc:  0.9160 F1-Mac:  0.8845 F1-Wei:  0.9149(165.014 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.1057% Dev Loss:  0.2404 Dev Acc:  0.8939 F1-Mac:  0.8657 F1-Wei:  0.8967(165.004 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.0706% Dev Loss:  0.2487 Dev Acc:  0.9160 F1-Mac:  0.8884 F1-Wei:  0.9164(164.978 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 91.7139% Dev Loss:  0.2752 Dev Acc:  0.9063 F1-Mac:  0.8819 F1-Wei:  0.9090(165.092 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.4184% Dev Loss:  0.2178 Dev Acc:  0.9187 F1-Mac:  0.8916 F1-Wei:  0.9190(165.124 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.1230% Dev Loss:  0.2614 Dev Acc:  0.9091 F1-Mac:  0.8843 F1-Wei:  0.9113(164.962 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 93.7050% Dev Loss:  0.3072 Dev Acc:  0.9229 F1-Mac:  0.8970 F1-Wei:  0.9230(165.013 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_01_010\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_01_010/claimspotter.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.19it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #1 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #2: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9671 9672 9673]\n",
      "INFO:absl:[   7    9   14 ... 9652 9658 9662]\n",
      "INFO:absl:[4642, 1887]\n",
      "INFO:absl:[1728, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:11<00:00,  2.85it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.4 Acc: 76.1066% Dev Loss:  0.2575 Dev Acc:  0.8994 F1-Mac:  0.8701 F1-Wei:  0.9002(205.077 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.44it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 87.2722% Dev Loss:  0.2265 Dev Acc:  0.9050 F1-Mac:  0.8719 F1-Wei:  0.9036(165.325 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.44it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -168.9 Acc: 88.6353% Dev Loss:  0.2097 Dev Acc:  0.9050 F1-Mac:  0.8714 F1-Wei:  0.9034(165.339 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.7 Acc: 89.4624% Dev Loss:  0.2081 Dev Acc:  0.9118 F1-Mac:  0.8786 F1-Wei:  0.9096(165.299 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.3661% Dev Loss:  0.2161 Dev Acc:  0.9077 F1-Mac:  0.8741 F1-Wei:  0.9058(165.122 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.2238% Dev Loss:  0.2218 Dev Acc:  0.9160 F1-Mac:  0.8876 F1-Wei:  0.9151(165.182 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 92.1274% Dev Loss:  0.2548 Dev Acc:  0.9105 F1-Mac:  0.8744 F1-Wei:  0.9072(165.362 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.44it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.6482% Dev Loss:  0.2307 Dev Acc:  0.9077 F1-Mac:  0.8808 F1-Wei:  0.9084(165.347 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.2608% Dev Loss:  0.2953 Dev Acc:  0.9077 F1-Mac:  0.8800 F1-Wei:  0.9081(165.164 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 93.9041% Dev Loss:  0.3554 Dev Acc:  0.8981 F1-Mac:  0.8613 F1-Wei:  0.8961(165.266 sec/epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_02_006\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_02_006/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:13<00:00,  7.23it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #2 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #3: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9670 9671 9673]\n",
      "INFO:absl:[   4   11   13 ... 9665 9667 9672]\n",
      "INFO:absl:[4678, 1852]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 545/545 [03:10<00:00,  2.86it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 78.2848% Dev Loss:  0.2672 Dev Acc:  0.8843 F1-Mac:  0.8577 F1-Wei:  0.8818(204.260 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 87.3354% Dev Loss:  0.2756 Dev Acc:  0.8747 F1-Mac:  0.8611 F1-Wei:  0.8781(165.223 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:39<00:00,  3.42it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -169.0 Acc: 89.1271% Dev Loss:  0.2196 Dev Acc:  0.9229 F1-Mac:  0.9082 F1-Wei:  0.9226(167.204 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.8 Acc: 89.4640% Dev Loss:  0.2175 Dev Acc:  0.9215 F1-Mac:  0.9062 F1-Wei:  0.9210(165.470 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.6738% Dev Loss:  0.2330 Dev Acc:  0.8939 F1-Mac:  0.8785 F1-Wei:  0.8954(165.179 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.3783% Dev Loss:  0.2232 Dev Acc:  0.9132 F1-Mac:  0.8972 F1-Wei:  0.9131(165.118 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 92.2052% Dev Loss:  0.2342 Dev Acc:  0.9132 F1-Mac:  0.8955 F1-Wei:  0.9123(165.195 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.8331% Dev Loss:  0.2312 Dev Acc:  0.9118 F1-Mac:  0.8909 F1-Wei:  0.9096(165.258 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.6141% Dev Loss:  0.2307 Dev Acc:  0.9105 F1-Mac:  0.8910 F1-Wei:  0.9090(165.227 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:40<00:00,  3.41it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 93.7979% Dev Loss:  0.2493 Dev Acc:  0.9215 F1-Mac:  0.9079 F1-Wei:  0.9217(168.445 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_03_010\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_03_010/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.03it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #3 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #4: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   1    2    3 ... 9670 9672 9673]\n",
      "INFO:absl:[   0    6   10 ... 9666 9668 9671]\n",
      "INFO:absl:[4648, 1882]\n",
      "INFO:absl:[1727, 691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:14<00:00,  2.80it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 80.3675% Dev Loss:  0.2600 Dev Acc:  0.8912 F1-Mac:  0.8639 F1-Wei:  0.8928(208.053 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 86.9985% Dev Loss:  0.2548 Dev Acc:  0.9118 F1-Mac:  0.8818 F1-Wei:  0.9100(165.287 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -168.9 Acc: 88.8821% Dev Loss:  0.2502 Dev Acc:  0.9187 F1-Mac:  0.8980 F1-Wei:  0.9198(165.170 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:37<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.8 Acc: 89.6325% Dev Loss:  0.2480 Dev Acc:  0.9146 F1-Mac:  0.8940 F1-Wei:  0.9161(165.133 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.1684% Dev Loss:  0.2368 Dev Acc:  0.9160 F1-Mac:  0.8907 F1-Wei:  0.9156(165.171 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.5927% Dev Loss:  0.2804 Dev Acc:  0.9132 F1-Mac:  0.8928 F1-Wei:  0.9150(165.214 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 91.5314% Dev Loss:  0.2829 Dev Acc:  0.9187 F1-Mac:  0.8954 F1-Wei:  0.9188(165.290 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.5727% Dev Loss:  0.3267 Dev Acc:  0.9132 F1-Mac:  0.8908 F1-Wei:  0.9142(165.341 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.45it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 92.8637% Dev Loss:  0.3397 Dev Acc:  0.9091 F1-Mac:  0.8843 F1-Wei:  0.9097(165.254 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:38<00:00,  3.44it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 93.5069% Dev Loss:  0.3225 Dev Acc:  0.9187 F1-Mac:  0.8932 F1-Wei:  0.9179(165.451 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_04_007\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_04_007/claimspotter.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:13<00:00,  7.23it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #4 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:[1. 1. 0. ... 0. 1. 0.]\n",
      "INFO:absl:[[ 0.32382396 -1.50450122]\n",
      " [-1.90801013  3.49868321]\n",
      " [ 1.29110825 -2.08472514]\n",
      " ...\n",
      " [-1.65627563  1.1161468 ]\n",
      " [-6.7592268   5.70051479]\n",
      " [ 4.06439257 -3.0374577 ]]\n",
      "INFO:absl:Final stats | F1-Mac: 0.8822 F1-Wei: 0.9040 nDCG: 0.9881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9310    0.9352    0.9331      6910\n",
      "         CFS     0.8361    0.8267    0.8314      2764\n",
      "\n",
      "    accuracy                         0.9042      9674\n",
      "   macro avg     0.8835    0.8809    0.8822      9674\n",
      "weighted avg     0.9039    0.9042    0.9040      9674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$BBA \\\n",
    "    --cs_adv_train=True \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=10 \\\n",
    "    --cs_batch_size_adv=12 \\\n",
    "    --cs_lambda=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGtvBK2PWU4c"
   },
   "source": [
    "Demonstration of the adversarially-trained BERT claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RM11sKu8WT-9"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$BBA \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQu21DUwzBpG"
   },
   "source": [
    "# DistilBERT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWwK8fqz0z8B"
   },
   "source": [
    "### Standard DistilBERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gB0jgLp_0zrL"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$DB \\\n",
    "    --cs_adv_train=False \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=30 \\\n",
    "    --cs_tfm_type=distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baxj_zOfWZ66"
   },
   "source": [
    "Demonstration of the standard DistilBERT claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7prejNw3WZzc"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$DB \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJxr69nq00Mx"
   },
   "source": [
    "### Adversarial DistilBERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJVwn81g0zbf"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$DBA \\\n",
    "    --cs_adv_train=True \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=35 \\\n",
    "    --cs_lambda=0.25 \\\n",
    "    --cs_tfm_type=distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7McP9TDQWfq8"
   },
   "source": [
    "Demonstration of the adversarially-trained DistilBERT claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f-Sa_M7WggY"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$DBA \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeVaGsS3y6qK"
   },
   "source": [
    "# RoBERTa models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UVpmdbMzeCd"
   },
   "source": [
    "### Standard RoBERTa model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOhHs93PzHBC"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$RB \\\n",
    "    --cs_adv_train=False \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=30 \\\n",
    "    --cs_tfm_type=roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0r30YxJWoLo"
   },
   "source": [
    "Demonstration of the standard RoBERTa claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ8oyUhTWoSL"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$RB \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxH-Sb6zjUB"
   },
   "source": [
    "### Adversarial RoBERTa model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kdAqq7SzmHl"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$RBA \\\n",
    "    --cs_adv_train=True \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=35 \\\n",
    "    --cs_lambda=0.25 \\\n",
    "    --cs_tfm_type=roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4YXoZAzWpFD"
   },
   "source": [
    "Demonstration of the adversarially-trained RoBERTa claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmMvXtQGWo-E"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$RBA \\\n",
    "    --cs_gpu=0 "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ClaimBuster_Adversarial_Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
