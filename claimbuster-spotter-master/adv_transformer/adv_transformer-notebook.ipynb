{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIsVQyq8hb_c"
   },
   "source": [
    "# Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYZlrsxTW3_T"
   },
   "source": [
    "Links to Google Colab containing other claim spotting models can be found below:\n",
    "\n",
    "\n",
    "*   [BiLSTM](https://colab.research.google.com/github/idirlab/claimspotter/blob/master/bidirectional_lstm/bilstm-notebook.ipynb)\n",
    "*   [SVM](https://colab.research.google.com/github/idirlab/claimspotter/blob/master/svm/svm-notebook.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZZ8f3i3hfYI"
   },
   "source": [
    "    Copyright (C) 2020 IDIR Lab - UT Arlington\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License v3 as published by\n",
    "    the Free Software Foundation.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "    Contact Information:\n",
    "    See: https://idir.uta.edu/cli.html\n",
    "\n",
    "    Chengkai Li\n",
    "    Box 19015\n",
    "    Arlington, TX 76019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERl7MzA-yumh"
   },
   "source": [
    "# Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0O8wRXsLe_8"
   },
   "outputs": [],
   "source": [
    "!git clone --branch develop_new_models https://github.com/idirlab/claimspotter.git\n",
    "!pip3 install transformers==3.5.1\n",
    "!pip3 install emoji\n",
    "\n",
    "%cd claimspotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWYfq1zZfyc5"
   },
   "source": [
    "## Model output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ilPe1se5gaiO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if \"adv_transformer\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"BB\"] = \"./output/bb/\"\n",
    "os.environ[\"BBA\"] = \"./output/bba/\"\n",
    "os.environ[\"DB\"] = \"./output/db/\"\n",
    "os.environ[\"DBA\"] = \"./output/dba/\"\n",
    "os.environ[\"RB\"] = \"./output/rb/\"\n",
    "os.environ[\"RBA\"] = \"./output/rba/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_PdF4zdy0re"
   },
   "source": [
    "# BERT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chgaME0gzUT9"
   },
   "source": [
    "### Standard BERT model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5DpLL03WJ-v"
   },
   "source": [
    "Demonstration of the BERT claimspotting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/fake-news-detection/Fake-news-detection/claimbuster-spotter-master\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-05-06 17:02:16.119719: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-05-06 17:02:16.140610: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199970000 Hz\n",
      "2024-05-06 17:02:16.140811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6acc000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-06 17:02:16.140835: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-06 17:02:16.143529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-06 17:02:16.412547: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b7d02cab70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-06 17:02:16.412586: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2024-05-06 17:02:16.412974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-06 17:02:16.413785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:02:16.417043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-06 17:02:16.420028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-06 17:02:16.421136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-06 17:02:16.424490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-06 17:02:16.426271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-06 17:02:16.432406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-06 17:02:16.433041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-06 17:02:16.433128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:02:16.433650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-06 17:02:16.433672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-05-06 17:02:16.433723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-05-06 17:02:16.434382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14890 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n",
      "INFO:absl:{'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'stderrthreshold': 'info', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': False, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/tmp/absl_testing', 'test_randomize_ordering_seed': '', 'xml_output_file': '', 'cs_gpu': ['0'], 'cs_ner_spacy': False, 'cs_model_dir': './output/bb/', 'cs_model_ckpt': 'claimspotter.ckpt', 'cs_data_dir': './data', 'cs_kfold_data_file': 'kfold_25ncs.json', 'cs_reg_train_file': 'train.json', 'cs_reg_test_file': 'test.json', 'cs_tb_dir': './tb_logs', 'cs_data_file_encoding': None, 'cs_use_clef_data': False, 'cs_refresh_data': True, 'cs_max_len': 200, 'cs_remove_stopwords': False, 'cs_sklearn_oversample': False, 'cs_weight_classes_loss': False, 'cs_custom_preprc': True, 'cs_random_state': 59, 'cs_num_classes': 2, 'cs_alt_two_class_combo': False, 'cs_stat_print_interval': 1, 'cs_model_save_interval': 1, 'cs_k_fold': 4, 'cs_restore_and_continue': False, 'cs_batch_size_reg': 24, 'cs_batch_size_adv': 12, 'cs_train_steps': 5, 'cs_lr': 5e-05, 'cs_l2_reg_coeff': 0.0, 'cs_adv_train': False, 'cs_perturb_id': 6, 'cs_adv_type': 0, 'cs_lambda': 0.25, 'cs_lambda_eps': 10.0, 'cs_combine_reg_adv_loss': True, 'cs_perturb_norm_length_range': [0.5, 5.0], 'cs_tfm_type': 'bert-base-uncased', 'cs_hidden_size': 768, 'cs_pool_strat': 'first', 'cs_tfm_ft_embed': False, 'cs_tfm_ft_pooler': True, 'cs_tfm_ft_enc_layers': 2, 'cs_kp_tfm_attn': 0.8, 'cs_kp_tfm_hidden': 0.8, 'cs_cls_hidden': 0, 'cs_kp_cls': 0.7, 'cs_custom_activation': True, 'cs_ca_r': 0.4, 'cs_model_loc': './data/bert-base-uncased_pretrain', 'cs_raw_kfold_data_loc': './data/two_class/kfold_25ncs.json', 'cs_raw_data_loc': './data/two_class/train.json', 'cs_raw_dj_eval_loc': './data/two_class/test.json', 'cs_raw_clef_train_loc': './data/clef20/task5/train.tsv', 'cs_raw_clef_test_loc': './data/clef20/task5/test-gold.tsv', 'cs_prc_data_loc': './data/all_data_bert-base-uncased.pickle', 'cs_prc_clef_loc': './data/all_clef_data.pickle'}\n",
      "INFO:absl:Loading dataset\n",
      "INFO:absl:./data/two_class/kfold_25ncs.json: [6910, 2764]\n",
      "INFO:absl:Loading preprocessing dependencies\n",
      "Dependencies Loaded.\n",
      "INFO:absl:Processing data\n",
      "100%|██████████████████████████████████████| 9674/9674 [00:25<00:00, 374.43it/s]\n",
      "INFO:absl:Refreshed data, successfully dumped at ./data/all_data_bert-base-uncased.pickle\n",
      "INFO:absl:Class weights computed to be [0.7  1.75]\n",
      "INFO:absl:9674 total cross-validation examples\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #1: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    4    6 ... 9668 9671 9672]\n",
      "INFO:absl:[   1    2    3 ... 9669 9670 9673]\n",
      "INFO:absl:[4636, 1893]\n",
      "INFO:absl:[1728, 691]\n",
      "2024-05-06 17:02:50.467232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-06 17:02:50.467400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:02:50.467445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-06 17:02:50.467587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-06 17:02:50.467629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-06 17:02:50.467662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-06 17:02:50.467693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-06 17:02:50.467772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-06 17:02:50.468325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-06 17:02:50.469319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-06 17:02:50.469402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:02:50.469438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-06 17:02:50.469474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-06 17:02:50.469504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-06 17:02:50.469583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-06 17:02:50.469616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-06 17:02:50.469643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-06 17:02:50.470170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-06 17:02:50.470256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-06 17:02:50.470273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-05-06 17:02:50.470282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-05-06 17:02:50.470915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14890 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n",
      "INFO:absl:Warming up...\n",
      "2024-05-06 17:03:11.553850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'dense/kernel:0', 'dense/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:50<00:00,  6.80it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:54<00:00,  5.00it/s]\n",
      "INFO:absl:Epoch   1 Loss:  0.5172 Acc: 75.4786% Dev Loss:  0.2773 Dev Acc:  0.8774 F1-Mac:  0.8395 F1-Wei:  0.8788(68.032 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.80it/s]\n",
      "INFO:absl:Epoch   2 Loss:   0.316 Acc: 86.9505% Dev Loss:  0.2338 Dev Acc:  0.9160 F1-Mac:  0.8845 F1-Wei:  0.9149(46.490 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2824 Acc: 88.5587% Dev Loss:  0.2603 Dev Acc:  0.8926 F1-Mac:  0.8642 F1-Wei:  0.8955(46.351 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2625 Acc: 89.2480% Dev Loss:  0.2171 Dev Acc:  0.9242 F1-Mac:  0.8963 F1-Wei:  0.9234(46.333 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2401 Acc: 90.1210% Dev Loss:  0.2373 Dev Acc:  0.9146 F1-Mac:  0.8828 F1-Wei:  0.9136(46.376 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_01_004\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_01_004/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.17it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9233    0.9468    0.9349      1728\n",
      "         CFS     0.8578    0.8032    0.8296       691\n",
      "\n",
      "    accuracy                         0.9057      2419\n",
      "   macro avg     0.8905    0.8750    0.8822      2419\n",
      "weighted avg     0.9046    0.9057    0.9048      2419\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #1 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #2: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9671 9672 9673]\n",
      "INFO:absl:[   7    9   14 ... 9652 9658 9662]\n",
      "INFO:absl:[4642, 1887]\n",
      "INFO:absl:[1728, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:47<00:00,  6.76it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:52<00:00,  5.25it/s]\n",
      "INFO:absl:Epoch   1 Loss:  0.5773 Acc: 71.8640% Dev Loss:  0.3236 Dev Acc:  0.8678 F1-Mac:  0.8240 F1-Wei:  0.8668(64.915 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3308 Acc: 86.4757% Dev Loss:  0.2399 Dev Acc:  0.8967 F1-Mac:  0.8656 F1-Wei:  0.8971(46.359 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2847 Acc: 88.1452% Dev Loss:  0.2342 Dev Acc:  0.8994 F1-Mac:  0.8607 F1-Wei:  0.8965(46.377 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.80it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2637 Acc: 89.1867% Dev Loss:  0.2179 Dev Acc:  0.8967 F1-Mac:  0.8661 F1-Wei:  0.8973(46.457 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.82it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2443 Acc: 90.0904% Dev Loss:  0.2383 Dev Acc:  0.9008 F1-Mac:  0.8769 F1-Wei:  0.9034(46.353 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_02_005\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_02_005/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:13<00:00,  7.25it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9576    0.8877    0.9213      1728\n",
      "         CFS     0.7625    0.9016    0.8263       691\n",
      "\n",
      "    accuracy                         0.8917      2419\n",
      "   macro avg     0.8600    0.8947    0.8738      2419\n",
      "weighted avg     0.9018    0.8917    0.8942      2419\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #2 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #3: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9670 9671 9673]\n",
      "INFO:absl:[   4   11   13 ... 9665 9667 9672]\n",
      "INFO:absl:[4678, 1852]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:47<00:00,  6.78it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:52<00:00,  5.22it/s]\n",
      "INFO:absl:Epoch   1 Loss:  0.5704 Acc: 73.0015% Dev Loss:  0.3130 Dev Acc:  0.8664 F1-Mac:  0.8344 F1-Wei:  0.8629(65.345 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.78it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3338 Acc: 86.3400% Dev Loss:  0.2784 Dev Acc:  0.8829 F1-Mac:  0.8553 F1-Wei:  0.8800(46.549 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.78it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2945 Acc: 87.7795% Dev Loss:  0.2833 Dev Acc:  0.8857 F1-Mac:  0.8710 F1-Wei:  0.8880(46.515 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.78it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2568 Acc: 89.3568% Dev Loss:  0.2954 Dev Acc:  0.8719 F1-Mac:  0.8596 F1-Wei:  0.8759(46.527 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.78it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2422 Acc: 90.2603% Dev Loss:  0.2372 Dev Acc:  0.9036 F1-Mac:  0.8853 F1-Wei:  0.9032(46.533 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_03_005\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_03_005/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.12it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9291    0.9415    0.9353      1727\n",
      "         CFS     0.8488    0.8205    0.8344       691\n",
      "\n",
      "    accuracy                         0.9069      2418\n",
      "   macro avg     0.8890    0.8810    0.8849      2418\n",
      "weighted avg     0.9062    0.9069    0.9065      2418\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #3 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #4: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   1    2    3 ... 9670 9672 9673]\n",
      "INFO:absl:[   0    6   10 ... 9666 9668 9671]\n",
      "INFO:absl:[4648, 1882]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting training...\n",
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|████████████████████████████████████████▊| 272/273 [00:47<00:00,  6.81it/s]WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adv_weights:0', 'adv_weights:0'] when minimizing the loss.\n",
      "100%|█████████████████████████████████████████| 273/273 [00:52<00:00,  5.24it/s]\n",
      "INFO:absl:Epoch   1 Loss:   0.502 Acc: 76.3706% Dev Loss:  0.2855 Dev Acc:  0.8939 F1-Mac:  0.8601 F1-Wei:  0.8927(64.935 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.80it/s]\n",
      "INFO:absl:Epoch   2 Loss:  0.3136 Acc: 87.2741% Dev Loss:  0.2917 Dev Acc:  0.8747 F1-Mac:  0.8507 F1-Wei:  0.8790(46.398 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   3 Loss:  0.2779 Acc: 88.3001% Dev Loss:  0.2635 Dev Acc:  0.9077 F1-Mac:  0.8791 F1-Wei:  0.9070(46.373 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.79it/s]\n",
      "INFO:absl:Epoch   4 Loss:  0.2511 Acc: 89.7090% Dev Loss:  0.2787 Dev Acc:  0.8953 F1-Mac:  0.8708 F1-Wei:  0.8975(46.462 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 273/273 [00:40<00:00,  6.81it/s]\n",
      "INFO:absl:Epoch   5 Loss:  0.2369 Acc: 90.9954% Dev Loss:  0.2684 Dev Acc:  0.9022 F1-Mac:  0.8799 F1-Wei:  0.9044(46.388 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bb/fold_04_005\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bb/fold_04_005/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:13<00:00,  7.31it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9607    0.8923    0.9252      1727\n",
      "         CFS     0.7715    0.9088    0.8346       691\n",
      "\n",
      "    accuracy                         0.8970      2418\n",
      "   macro avg     0.8661    0.9006    0.8799      2418\n",
      "weighted avg     0.9066    0.8970    0.8993      2418\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #4 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:[1. 0. 0. ... 0. 1. 0.]\n",
      "INFO:absl:[[-0.42106438  0.8569957 ]\n",
      " [ 0.44944796  0.09871078]\n",
      " [ 0.44535679  0.20412883]\n",
      " ...\n",
      " [ 2.94866586 -2.93856883]\n",
      " [-3.19169927  3.55077457]\n",
      " [ 3.35113549 -3.40964365]]\n",
      "INFO:absl:Final stats | F1-Mac: 0.8802 F1-Wei: 0.9013 nDCG: 0.9883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9419    0.9171    0.9293      6910\n",
      "         CFS     0.8055    0.8585    0.8312      2764\n",
      "\n",
      "    accuracy                         0.9004      9674\n",
      "   macro avg     0.8737    0.8878    0.8802      9674\n",
      "weighted avg     0.9029    0.9004    0.9013      9674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$BB \\\n",
    "    --cs_adv_train=False \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=5 \\\n",
    "    --cs_refresh_data=True \\\n",
    "    # --cs_kfold_data_file='checkthat_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73EtHQT9uMCP"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$BB \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RaCy2vEzbTL"
   },
   "source": [
    "### Adversarial BERT model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pmBLZuntzTc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-05-06 17:57:49.512312: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-05-06 17:57:49.533605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199970000 Hz\n",
      "2024-05-06 17:57:49.533808: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1600000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-06 17:57:49.533833: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-06 17:57:49.536557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-06 17:57:49.816851: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c58c630b10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-06 17:57:49.816889: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2024-05-06 17:57:49.818178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-06 17:57:49.819019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:57:49.822242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-06 17:57:49.825157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-06 17:57:49.905644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-06 17:57:50.292110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-06 17:57:50.529379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-06 17:57:50.891286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-06 17:57:50.891987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-06 17:57:50.892083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:57:50.892633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-06 17:57:50.892684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-05-06 17:57:50.892703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-05-06 17:57:50.893441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14890 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n",
      "INFO:absl:{'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'stderrthreshold': 'info', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': False, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/tmp/absl_testing', 'test_randomize_ordering_seed': '', 'xml_output_file': '', 'cs_gpu': ['0'], 'cs_ner_spacy': False, 'cs_model_dir': './output/bba/', 'cs_model_ckpt': 'claimspotter.ckpt', 'cs_data_dir': './data', 'cs_kfold_data_file': 'kfold_25ncs.json', 'cs_reg_train_file': 'train.json', 'cs_reg_test_file': 'test.json', 'cs_tb_dir': './tb_logs', 'cs_data_file_encoding': None, 'cs_use_clef_data': False, 'cs_refresh_data': True, 'cs_max_len': 200, 'cs_remove_stopwords': False, 'cs_sklearn_oversample': False, 'cs_weight_classes_loss': False, 'cs_custom_preprc': True, 'cs_random_state': 59, 'cs_num_classes': 2, 'cs_alt_two_class_combo': False, 'cs_stat_print_interval': 1, 'cs_model_save_interval': 1, 'cs_k_fold': 4, 'cs_restore_and_continue': False, 'cs_batch_size_reg': 24, 'cs_batch_size_adv': 12, 'cs_train_steps': 10, 'cs_lr': 5e-05, 'cs_l2_reg_coeff': 0.0, 'cs_adv_train': True, 'cs_perturb_id': 6, 'cs_adv_type': 0, 'cs_lambda': 0.1, 'cs_lambda_eps': 10.0, 'cs_combine_reg_adv_loss': True, 'cs_perturb_norm_length_range': [0.5, 5.0], 'cs_tfm_type': 'bert-base-uncased', 'cs_hidden_size': 768, 'cs_pool_strat': 'first', 'cs_tfm_ft_embed': False, 'cs_tfm_ft_pooler': True, 'cs_tfm_ft_enc_layers': 2, 'cs_kp_tfm_attn': 0.8, 'cs_kp_tfm_hidden': 0.8, 'cs_cls_hidden': 0, 'cs_kp_cls': 0.7, 'cs_custom_activation': True, 'cs_ca_r': 0.4, 'cs_model_loc': './data/bert-base-uncased_pretrain', 'cs_raw_kfold_data_loc': './data/two_class/kfold_25ncs.json', 'cs_raw_data_loc': './data/two_class/train.json', 'cs_raw_dj_eval_loc': './data/two_class/test.json', 'cs_raw_clef_train_loc': './data/clef20/task5/train.tsv', 'cs_raw_clef_test_loc': './data/clef20/task5/test-gold.tsv', 'cs_prc_data_loc': './data/all_data_bert-base-uncased.pickle', 'cs_prc_clef_loc': './data/all_clef_data.pickle'}\n",
      "INFO:absl:Loading dataset\n",
      "INFO:absl:./data/two_class/kfold_25ncs.json: [6910, 2764]\n",
      "INFO:absl:Loading preprocessing dependencies\n",
      "Dependencies Loaded.\n",
      "INFO:absl:Processing data\n",
      "100%|██████████████████████████████████████| 9674/9674 [00:25<00:00, 377.90it/s]\n",
      "INFO:absl:Refreshed data, successfully dumped at ./data/all_data_bert-base-uncased.pickle\n",
      "INFO:absl:Class weights computed to be [0.7  1.75]\n",
      "INFO:absl:9674 total cross-validation examples\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #1: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    4    6 ... 9668 9671 9672]\n",
      "INFO:absl:[   1    2    3 ... 9669 9670 9673]\n",
      "INFO:absl:[4636, 1893]\n",
      "INFO:absl:[1728, 691]\n",
      "2024-05-06 17:58:24.223600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-06 17:58:24.223764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:58:24.223806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-06 17:58:24.223900: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-06 17:58:24.223941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-06 17:58:24.223974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-06 17:58:24.224004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-06 17:58:24.224075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-06 17:58:24.224637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-06 17:58:24.225701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-06 17:58:24.225783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-06 17:58:24.225817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-06 17:58:24.225843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-06 17:58:24.225868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-06 17:58:24.225894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-06 17:58:24.225974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-06 17:58:24.226004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-06 17:58:24.226541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-06 17:58:24.226628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-06 17:58:24.226644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-05-06 17:58:24.226653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-05-06 17:58:24.227253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14890 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n",
      "INFO:absl:Warming up...\n",
      "2024-05-06 17:58:26.109389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'dense/kernel:0', 'dense/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:09<00:00,  2.87it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 78.1743% Dev Loss:  0.2720 Dev Acc:  0.8926 F1-Mac:  0.8586 F1-Wei:  0.8935(204.142 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 87.3181% Dev Loss:  0.2356 Dev Acc:  0.9036 F1-Mac:  0.8707 F1-Wei:  0.9036(161.966 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:35<00:00,  3.51it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -168.9 Acc: 88.9110% Dev Loss:  0.2113 Dev Acc:  0.9050 F1-Mac:  0.8785 F1-Wei:  0.9071(162.018 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.7 Acc: 89.9525% Dev Loss:  0.2166 Dev Acc:  0.9118 F1-Mac:  0.8843 F1-Wei:  0.9128(161.504 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.0 Acc: 90.2588% Dev Loss:  0.2194 Dev Acc:  0.9187 F1-Mac:  0.8928 F1-Wei:  0.9194(161.503 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:35<00:00,  3.51it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.2544% Dev Loss:  0.2014 Dev Acc:  0.9174 F1-Mac:  0.8884 F1-Wei:  0.9170(162.473 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 91.3310% Dev Loss:  0.1991 Dev Acc:  0.9201 F1-Mac:  0.8937 F1-Wei:  0.9204(161.904 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.5869% Dev Loss:  0.2278 Dev Acc:  0.9270 F1-Mac:  0.9044 F1-Wei:  0.9278(161.945 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.2302% Dev Loss:  0.2782 Dev Acc:  0.9215 F1-Mac:  0.8989 F1-Wei:  0.9230(161.832 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 93.6131% Dev Loss:  0.2965 Dev Acc:  0.9063 F1-Mac:  0.8816 F1-Wei:  0.9089(162.020 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_01_008\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_01_008/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:13<00:00,  7.29it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9397    0.9201    0.9298      1728\n",
      "         CFS     0.8102    0.8524    0.8307       691\n",
      "\n",
      "    accuracy                         0.9008      2419\n",
      "   macro avg     0.8749    0.8863    0.8803      2419\n",
      "weighted avg     0.9027    0.9008    0.9015      2419\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #1 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #2: 6529 train 726 val 2419 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9671 9672 9673]\n",
      "INFO:absl:[   7    9   14 ... 9652 9658 9662]\n",
      "INFO:absl:[4642, 1887]\n",
      "INFO:absl:[1728, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:08<00:00,  2.90it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 79.9050% Dev Loss:  0.2449 Dev Acc:  0.9008 F1-Mac:  0.8708 F1-Wei:  0.9012(201.734 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 88.0533% Dev Loss:  0.2794 Dev Acc:  0.8994 F1-Mac:  0.8520 F1-Wei:  0.8929(161.977 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -169.0 Acc: 89.4471% Dev Loss:  0.1956 Dev Acc:  0.9132 F1-Mac:  0.8860 F1-Wei:  0.9131(161.555 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.8 Acc: 90.4120% Dev Loss:  0.1982 Dev Acc:  0.9174 F1-Mac:  0.8900 F1-Wei:  0.9168(161.470 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.1210% Dev Loss:  0.2207 Dev Acc:  0.9036 F1-Mac:  0.8800 F1-Wei:  0.9059(161.539 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.9283% Dev Loss:  0.1946 Dev Acc:  0.9284 F1-Mac:  0.9054 F1-Wei:  0.9281(161.502 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 92.1887% Dev Loss:  0.1942 Dev Acc:  0.9256 F1-Mac:  0.9028 F1-Wei:  0.9257(161.442 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.7860% Dev Loss:  0.2227 Dev Acc:  0.9187 F1-Mac:  0.8878 F1-Wei:  0.9165(161.546 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.4906% Dev Loss:  0.2404 Dev Acc:  0.9187 F1-Mac:  0.8980 F1-Wei:  0.9204(161.522 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 94.2258% Dev Loss:  0.2175 Dev Acc:  0.9229 F1-Mac:  0.8954 F1-Wei:  0.9215(161.450 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_3/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_02_006\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_02_006/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.18it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9359    0.9381    0.9370      1728\n",
      "         CFS     0.8443    0.8394    0.8418       691\n",
      "\n",
      "    accuracy                         0.9099      2419\n",
      "   macro avg     0.8901    0.8887    0.8894      2419\n",
      "weighted avg     0.9097    0.9099    0.9098      2419\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #2 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #3: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   0    1    2 ... 9670 9671 9673]\n",
      "INFO:absl:[   4   11   13 ... 9665 9667 9672]\n",
      "INFO:absl:[4678, 1852]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_4/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:07<00:00,  2.90it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 80.5207% Dev Loss:  0.2890 Dev Acc:  0.8953 F1-Mac:  0.8676 F1-Wei:  0.8913(201.203 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:35<00:00,  3.51it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 87.9632% Dev Loss:  0.2315 Dev Acc:  0.9008 F1-Mac:  0.8865 F1-Wei:  0.9023(162.192 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:36<00:00,  3.48it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -169.0 Acc: 89.2343% Dev Loss:  0.2085 Dev Acc:  0.9174 F1-Mac:  0.8998 F1-Wei:  0.9162(165.614 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.8 Acc: 89.8775% Dev Loss:  0.2304 Dev Acc:  0.8981 F1-Mac:  0.8836 F1-Wei:  0.8997(162.290 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.6585% Dev Loss:  0.2359 Dev Acc:  0.9201 F1-Mac:  0.9031 F1-Wei:  0.9190(161.741 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 90.9342% Dev Loss:  0.2499 Dev Acc:  0.9146 F1-Mac:  0.8970 F1-Wei:  0.9137(161.655 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:35<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 92.1286% Dev Loss:  0.3176 Dev Acc:  0.8760 F1-Mac:  0.8635 F1-Wei:  0.8797(161.972 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 92.4043% Dev Loss:  0.2544 Dev Acc:  0.9091 F1-Mac:  0.8898 F1-Wei:  0.9078(161.931 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.0781% Dev Loss:  0.2620 Dev Acc:  0.8953 F1-Mac:  0.8779 F1-Wei:  0.8960(162.032 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 94.1654% Dev Loss:  0.3023 Dev Acc:  0.8926 F1-Mac:  0.8770 F1-Wei:  0.8941(161.910 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_5/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_03_006\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_03_006/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:13<00:00,  7.26it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9301    0.9473    0.9386      1727\n",
      "         CFS     0.8619    0.8220    0.8415       691\n",
      "\n",
      "    accuracy                         0.9115      2418\n",
      "   macro avg     0.8960    0.8847    0.8900      2418\n",
      "weighted avg     0.9106    0.9115    0.9109      2418\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #3 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:|     Running k-fold cross-val iteration #4: 6530 train 726 val 2418 test     |\n",
      "INFO:absl:|                                                                             |\n",
      "INFO:absl:-------------------------------------------------------------------------------\n",
      "INFO:absl:[   1    2    3 ... 9670 9672 9673]\n",
      "INFO:absl:[   0    6   10 ... 9666 9668 9671]\n",
      "INFO:absl:[4648, 1882]\n",
      "INFO:absl:[1727, 691]\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_6/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Starting adversarial training...\n",
      "100%|█████████████████████████████████████████| 545/545 [03:07<00:00,  2.90it/s]\n",
      "INFO:absl:Epoch   1 Loss:  -143.5 Acc: 80.2297% Dev Loss:  0.2569 Dev Acc:  0.8967 F1-Mac:  0.8656 F1-Wei:  0.8962(201.397 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:35<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   2 Loss:  -155.9 Acc: 87.4119% Dev Loss:  0.2774 Dev Acc:  0.8939 F1-Mac:  0.8701 F1-Wei:  0.8965(162.013 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   3 Loss:  -168.9 Acc: 89.2343% Dev Loss:  0.2647 Dev Acc:  0.8994 F1-Mac:  0.8629 F1-Wei:  0.8964(161.668 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   4 Loss:  -182.7 Acc: 89.6325% Dev Loss:  0.2622 Dev Acc:  0.9091 F1-Mac:  0.8861 F1-Wei:  0.9104(161.653 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   5 Loss:  -197.1 Acc: 90.7810% Dev Loss:  0.2919 Dev Acc:  0.9022 F1-Mac:  0.8777 F1-Wei:  0.9036(161.655 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   6 Loss:  -211.7 Acc: 91.2711% Dev Loss:  0.3123 Dev Acc:  0.8898 F1-Mac:  0.8636 F1-Wei:  0.8919(161.615 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.53it/s]\n",
      "INFO:absl:Epoch   7 Loss:  -226.5 Acc: 91.2711% Dev Loss:  0.2742 Dev Acc:  0.9105 F1-Mac:  0.8847 F1-Wei:  0.9105(161.517 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   8 Loss:  -241.5 Acc: 93.0168% Dev Loss:  0.3304 Dev Acc:  0.9105 F1-Mac:  0.8862 F1-Wei:  0.9111(161.635 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch   9 Loss:  -256.5 Acc: 93.2619% Dev Loss:  0.3148 Dev Acc:  0.9118 F1-Mac:  0.8871 F1-Wei:  0.9121(161.693 sec/epoch)\n",
      "100%|█████████████████████████████████████████| 545/545 [02:34<00:00,  3.52it/s]\n",
      "INFO:absl:Epoch  10 Loss:  -271.5 Acc: 93.9204% Dev Loss:  0.3057 Dev Acc:  0.9229 F1-Mac:  0.8992 F1-Wei:  0.9223(161.621 sec/epoch)\n",
      "INFO:absl:Warming up...\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model_7/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'adv_weights:0']\n",
      "INFO:absl:Attempting to restore weights from ./output/bba/fold_04_010\n",
      "INFO:absl:Retrieving pre-trained weights from ./output/bba/fold_04_010/claimspotter.ckpt\n",
      "INFO:absl:Restore successful\n",
      "INFO:absl:Starting evaluation...\n",
      "100%|█████████████████████████████████████████| 101/101 [00:14<00:00,  7.11it/s]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer.optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9225    0.9444    0.9333      1727\n",
      "         CFS     0.8523    0.8017    0.8262       691\n",
      "\n",
      "    accuracy                         0.9036      2418\n",
      "   macro avg     0.8874    0.8731    0.8798      2418\n",
      "weighted avg     0.9024    0.9036    0.9027      2418\n",
      "\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:|     Iteration #4 OK     |\n",
      "INFO:absl:|                         |\n",
      "INFO:absl:---------------------------\n",
      "INFO:absl:[0. 1. 0. ... 1. 0. 1.]\n",
      "INFO:absl:[[ 1.90443492 -2.1379137 ]\n",
      " [-3.66185045  4.77213001]\n",
      " [ 0.48779789 -0.5869602 ]\n",
      " ...\n",
      " [-3.66232228  2.65455914]\n",
      " [ 5.22045279 -4.90984201]\n",
      " [-0.44935516 -0.53278071]]\n",
      "INFO:absl:Final stats | F1-Mac: 0.8849 F1-Wei: 0.9062 nDCG: 0.9884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NCS     0.9320    0.9375    0.9347      6910\n",
      "         CFS     0.8414    0.8289    0.8351      2764\n",
      "\n",
      "    accuracy                         0.9065      9674\n",
      "   macro avg     0.8867    0.8832    0.8849      9674\n",
      "weighted avg     0.9061    0.9065    0.9062      9674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$BBA \\\n",
    "    --cs_adv_train=True \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=10 \\\n",
    "    --cs_batch_size_adv=12 \\\n",
    "    --cs_lambda=0.1 \\\n",
    "    --cs_refresh_data=True \\\n",
    "    # --cs_kfold_data_file='checkthat_dataset.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGtvBK2PWU4c"
   },
   "source": [
    "Demonstration of the adversarially-trained BERT claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RM11sKu8WT-9"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$BBA \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQu21DUwzBpG"
   },
   "source": [
    "# DistilBERT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWwK8fqz0z8B"
   },
   "source": [
    "### Standard DistilBERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gB0jgLp_0zrL"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$DB \\\n",
    "    --cs_adv_train=False \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=30 \\\n",
    "    --cs_tfm_type=distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baxj_zOfWZ66"
   },
   "source": [
    "Demonstration of the standard DistilBERT claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7prejNw3WZzc"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$DB \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJxr69nq00Mx"
   },
   "source": [
    "### Adversarial DistilBERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJVwn81g0zbf"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$DBA \\\n",
    "    --cs_adv_train=True \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=35 \\\n",
    "    --cs_lambda=0.25 \\\n",
    "    --cs_tfm_type=distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7McP9TDQWfq8"
   },
   "source": [
    "Demonstration of the adversarially-trained DistilBERT claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f-Sa_M7WggY"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$DBA \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeVaGsS3y6qK"
   },
   "source": [
    "# RoBERTa models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UVpmdbMzeCd"
   },
   "source": [
    "### Standard RoBERTa model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOhHs93PzHBC"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$RB \\\n",
    "    --cs_adv_train=False \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=30 \\\n",
    "    --cs_tfm_type=roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0r30YxJWoLo"
   },
   "source": [
    "Demonstration of the standard RoBERTa claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ8oyUhTWoSL"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$RB \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxH-Sb6zjUB"
   },
   "source": [
    "### Adversarial RoBERTa model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kdAqq7SzmHl"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.train \\\n",
    "    --cs_model_dir=$RBA \\\n",
    "    --cs_adv_train=True \\\n",
    "    --cs_gpu=0 \\\n",
    "    --cs_train_steps=35 \\\n",
    "    --cs_lambda=0.25 \\\n",
    "    --cs_tfm_type=roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4YXoZAzWpFD"
   },
   "source": [
    "Demonstration of the adversarially-trained RoBERTa claimspotting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmMvXtQGWo-E"
   },
   "outputs": [],
   "source": [
    "!python3 -m adv_transformer.demo \\\n",
    "    --cs_model_dir=$RBA \\\n",
    "    --cs_gpu=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/matssbra/fake-news-detection/Fake-news-detection/claimbuster-spotter-master\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cluster/home/matssbra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:absl:Restoring data from ./data/all_data_bert-base-uncased.pickle\n",
      "INFO:absl:Class weights computed to be [0.7  1.75]\n",
      "2024-05-12 15:12:46.476886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-12 15:12:46.657350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.74GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-12 15:12:46.661247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-12 15:12:46.670710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-12 15:12:46.681886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-12 15:12:46.692666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-12 15:12:46.695824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-12 15:12:46.702457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-12 15:12:46.708184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-12 15:12:46.720149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-12 15:12:46.720707: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-05-12 15:12:46.746252: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199875000 Hz\n",
      "2024-05-12 15:12:46.748964: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff1c4000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-12 15:12:46.749031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-12 15:12:47.545605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5593ace30ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-12 15:12:47.545642: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2024-05-12 15:12:47.546038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.74GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-05-12 15:12:47.546107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-12 15:12:47.546244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2024-05-12 15:12:47.546274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-12 15:12:47.546299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-12 15:12:47.546324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-05-12 15:12:47.546349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-05-12 15:12:47.546380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-05-12 15:12:47.547947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2024-05-12 15:12:47.548016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-05-12 15:12:47.548384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-12 15:12:47.548404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2024-05-12 15:12:47.548416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2024-05-12 15:12:47.548942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30259 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0)\n",
      "2024-05-12 15:12:52.147201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Removing: ['_._0/', '_._1/', '_._2/', '_._3/', '_._4/', '_._5/', '_._6/', '_._7/', '_._8/', '_._9/', '/embeddings/']\n",
      "INFO:absl:Trainable variables: ['adv_weights:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'dense/kernel:0', 'dense/bias:0', 'adv_weights:0']\n",
      "21.62245128164068\n"
     ]
    }
   ],
   "source": [
    "%cd /cluster/home/matssbra/fake-news-detection/Fake-news-detection/claimbuster-spotter-master\n",
    "!python3 -m adv_transformer.runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ClaimBuster_Adversarial_Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
